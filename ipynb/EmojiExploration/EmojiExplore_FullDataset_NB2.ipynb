{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/GA_DSI/Projects/capstone\n"
     ]
    }
   ],
   "source": [
    "cd /home/jovyan/GA_DSI/Projects/capstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lib.general_utilities as gu\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report,precision_recall_curve,confusion_matrix \n",
    "from sklearn.metrics import (precision_score,accuracy_score,roc_auc_score,roc_curve, \n",
    "                             precision_recall_curve,recall_score,make_scorer,auc)\n",
    "from tqdm import tqdm\n",
    "\n",
    "# storing data directories for this notebook.\n",
    "img_out_dir = 'data/images/Emoticon_NB4/'\n",
    "data_out_dir = 'data/pickled/Emoticon_NB4/'\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full original dataset dataset.\n",
    "air_df = pd.read_csv(\"data/csvfiles/kaggle_airline_dataset.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 15)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude name of airlines from the tweets. \n",
    "exclude = ['virginamerica', 'united', 'southwestair', 'delta', 'usairways',\n",
    "           'americanair', 'jetblue', 'southwest', 'flight', 'flights',\n",
    "           'URL', 'AT_USER', 'amp', 'amp;']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stop words. Use the defined 'english' stop words. I will add the airline names to that.\n",
    "stop_words = list(ENGLISH_STOP_WORDS)\n",
    "stop_words.extend(exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_emojis(text):\n",
    "    input_txt = text\n",
    "    for word in text:\n",
    "        if word in emoji.UNICODE_EMOJI:\n",
    "            input_txt = input_txt.replace(word, emoji_dict[word])\n",
    "    return(input_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emojis(text):\n",
    "    return(' '.join(word for word in text if word in emoji.UNICODE_EMOJI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column of only emojis for that text.\n",
    "air_df['emojis'] = air_df['text'].apply(extract_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = air_df[['emojis', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emojis</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>❤ ☺ 👍</td>\n",
       "      <td>I ❤️ flying @VirginAmerica. ☺️👍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>😡</td>\n",
       "      <td>@VirginAmerica you guys messed up my seating.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>😢</td>\n",
       "      <td>@VirginAmerica hi! I just bked a cool birthday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>💜 ✈</td>\n",
       "      <td>@VirginAmerica Moodlighting is the only way to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>🍷 👍 💺 ✈</td>\n",
       "      <td>@VirginAmerica plz help me win my bid upgrade ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>😊</td>\n",
       "      <td>@VirginAmerica @ladygaga @carrieunderwood  I'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>😍 👌</td>\n",
       "      <td>@VirginAmerica @ladygaga @carrieunderwood all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>💕 💕</td>\n",
       "      <td>@VirginAmerica - amazing customer  service, ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>🌞 ✈</td>\n",
       "      <td>@VirginAmerica Have a great week 🌞✈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>😃</td>\n",
       "      <td>@VirginAmerica Can you find us a flt out of LA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      emojis                                               text\n",
       "18     ❤ ☺ 👍                    I ❤️ flying @VirginAmerica. ☺️👍\n",
       "24         😡  @VirginAmerica you guys messed up my seating.....\n",
       "30         😢  @VirginAmerica hi! I just bked a cool birthday...\n",
       "36       💜 ✈  @VirginAmerica Moodlighting is the only way to...\n",
       "42   🍷 👍 💺 ✈  @VirginAmerica plz help me win my bid upgrade ...\n",
       "57         😊  @VirginAmerica @ladygaga @carrieunderwood  I'm...\n",
       "62       😍 👌  @VirginAmerica @ladygaga @carrieunderwood all ...\n",
       "105      💕 💕  @VirginAmerica - amazing customer  service, ag...\n",
       "113      🌞 ✈                @VirginAmerica Have a great week 🌞✈\n",
       "142        😃  @VirginAmerica Can you find us a flt out of LA..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.emojis != ''][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '❤ ☺ 👍', '😡', '😢', '💜 ✈', '🍷 👍 💺 ✈', '😊', '😍 👌', '💕 💕', '🌞 ✈',\n",
       "       '😃', '😩 😭', '😎', '🙉', '😁', '❤', '❄ ❄ ❄', '👏', '😂 💗', '🍸', '😒', '👎',\n",
       "       '👍 👍 ✈ ✈ 💗', '😊 😀 😃 😄', '😘', '👸 💗', '😁 😁 😁', '😍 💗 💗 💗 🎀 🌏 🎀', '😥',\n",
       "       '🎀 🎀 🎀', '😉', '😍 😍 ✨ 😱 ✈', '✈ 🎉', '💗 🎀 💗', '😃 👍', '🙌 💤', '😞', '♥',\n",
       "       '👋', '✌', '🙏', '🙌', '💜', '✈', '❄ ❄', '😃 😃 😃', '👿', '😉 😉', '😔', '😭',\n",
       "       '🙌 🙌', '🙅 🙅 🙅', '😉 😉 😉', '😡 😡', '👏 👏 👏 ✈', '👍', '🆖', '💩', '✔',\n",
       "       '🌴 🌴', '✅ ❌', '👏 👏 👏 👏 👏 👏', '👏 👏', '😄 😄 😄 😡 😡 😡', '👠 👠 👠', '😜',\n",
       "       '🎉 🎉 🎉', '😻 😻 😻', '😊 😕', '👌', '👎 👎', '😈', '😡 😡 😡 😡 😤 😤 😤', '❄',\n",
       "       '👏 👏 👏', '👍 😊', '👍 👌', '😂', '😀', '💪', '😩 😫 😢', '💔 😪', '😕', '😣', '😬',\n",
       "       '😄', '😂 💁', '😋', '🙌 😏', '😖', '🌟 🌟', '✈ 📱', '👏 👏 🍻 🍻', '😞 😡', '💖',\n",
       "       '😂 😂 😂 😂 😂 😭 😭 😭 😭 😢 😢 😢 😢', '😅', '😔 😔 😔', '💝', '🚫 ↔', '😏', '😜 😂',\n",
       "       '😷 😱', '⭐ ✈', '😃 💕 🎵 ✈ ❗ ❤', '😤 🐴', '😍', '😭 😭 😭',\n",
       "       '😭 😭 💔 💔 💔 💔 💔 💔 💔', '😂 😂', '☺', '💁', '😭 🙏', '😆', '😊 🌴', '👍 👍',\n",
       "       '✈ ✈', '♥ 🙏', '✈ 😃 👍', '❤ ❤ ❤', '😩', '😑', '💕', '🐩', '😃 💕 😍 ⤴ ⤴',\n",
       "       '😒 👎', '☀', '😜 😎', '❤ 👊', '😱 ❤', '😁 😁 😁 😁', '😭 😭', '💯', '💩 💩 💩 💩',\n",
       "       '😠 😠', '❤ ❤ ❤ 😍 🌏', '✈ 💺', '😜 ✈', '😊 ☕ 📲 ✈', '😠', '👺', '🙈', '💘',\n",
       "       '👏 👏 👏 👏', '💙', '👉 🚪', '🙅', '😳', '😭 😁 😆 😵', '😂 😂 😂', '😂 😂 😂 😂',\n",
       "       '🚶 🚶 🚶 🚶', '✈ 🔵 🔵 🔵', '😩 😩 😩', '😐 😑', '👀 👀', '😂 😂 😭 😭 😭 😭', '👀',\n",
       "       '🍅 🍅 🍅 🍅', '😐', '😊 😊', '💙 💙 💙 💙', '😂 👌 👌 👌', '💯 💯 💯', '😩 😂 😂 😂 😂',\n",
       "       '😍 😍 😍', '🙌 ✈', '🌴', '☺ ✈', '☺ 👍 👍', '😭 😭 😭 😭', '✈ ☺', '😁 🎉',\n",
       "       '🆘 🆘 🆘 🆘 🆘 🆘 🆘 🆘 🆘 🆘 🆘 🆘 🆘 🆘', '❄ ⛄', '👍 👍 😊', '🌞 ✈ 👸', '💔', '😷',\n",
       "       '😒 👺', '😘 🙌', '💙 💙', '😊 ✈', '❤ ✨', '☕ ✈ 👍', '😢 😢', '🍷',\n",
       "       '😊 😊 😊 ✈ ✈ ✈', '😂 😂 😂 😂 😂', '👌 ☺', '😏 ☺', '🙏 🙏 ❤ 😏', '🙏 ❤', '😢 😢 😢',\n",
       "       '👍 👍 👍', '💕 ✈ 💺', '😔 😓 😤', '☀ 🌴 ✈ 🍸 🎲', '👏 👍', '⭐ ⭐ ⭐ ⭐ ⭐', '🙏 🙏 🙏',\n",
       "       '😡 😡 😡', '⌚', '👊', '😑 😩', '🐳', '⛄ ☀', '💺 ✈', '🙏 🙏 🙏 ✌ ✌ ✌ 🙏 🙏 🙏',\n",
       "       '⤵', '🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏', '👎 ✈ 👎 😬 👎 😡 😊 👏', '😤',\n",
       "       '🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 😢 😢 😢 😢 😢 😢 😢 😢 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏',\n",
       "       '🙏 🙏 🙏 😢 😢 😢 🙏 🙏 🙏',\n",
       "       '🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏 🙏',\n",
       "       '🙏 🙏 🙏 ✌ 😉', '✈ 🌞', '😮', '👎 😡 ✈', '👌 👌 👌', '😩 😭 💔', '👍 👍 👍 👍 👍',\n",
       "       '😫 😫 😫', '😥 😥', '😄 💝 💝 💝', '😡 😤 😖 😲 😩', '😘 😑', '😢 😕 😦', '❤ ❤',\n",
       "       '😓 😭', '➡ 😕', '✈ ✈ ✈ ✈ ✈ ✈ ✈', '✈ ✌'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_emoji_strings = df.emojis.unique()\n",
    "unique_emoji_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***** Unique list of emoji features. *****\n",
    "uni_emoji_feat_lst = unique_emoji_strings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '❤ ☺ 👍', '😡', '😢', '💜 ✈', '🍷 👍 💺 ✈', '😊', '😍 👌', '💕 💕', '🌞 ✈']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_emoji_feat_lst[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a list of unique single emojis found in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_single_emoji=[]\n",
    "for emoj_str in uni_emoji_feat_lst:\n",
    "    emoj_lst = emoj_str.split()\n",
    "    [uni_single_emoji.append(em) for em in emoj_lst if (em not in uni_single_emoji)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dataset subsetted to where the sentiment confidence level was bove 70 percent, has 17 less\n",
    "# emojis than the full dataset. I am creating a comprehensive dictionary now so I can run the full\n",
    "# dataset.\n",
    "len(uni_single_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['❤', '☺', '👍', '😡', '😢', '💜', '✈', '🍷', '💺', '😊', '😍', '👌', '💕', '🌞', '😃', '😩', '😭', '😎', '🙉', '😁', '❄', '👏', '😂', '💗', '🍸', '😒', '👎', '😀', '😄', '😘', '👸', '🎀', '🌏', '😥', '😉', '✨', '😱', '🎉', '🙌', '💤', '😞', '♥', '👋', '✌', '🙏', '👿', '😔', '🙅', '🆖', '💩', '✔', '🌴', '✅', '❌', '👠', '😜', '😻', '😕', '😈', '😤', '💪', '😫', '💔', '😪', '😣', '😬', '💁', '😋', '😏', '😖', '🌟', '📱', '🍻', '💖', '😅', '💝', '🚫', '↔', '😷', '⭐', '🎵', '❗', '🐴', '😆', '😑', '🐩', '⤴', '☀', '👊', '💯', '😠', '☕', '📲', '👺', '🙈', '💘', '💙', '👉', '🚪', '😳', '😵', '🚶', '🔵', '😐', '👀', '🍅', '🆘', '⛄', '😓', '🎲', '⌚', '🐳', '⤵', '😮', '😲', '😦', '➡']\n"
     ]
    }
   ],
   "source": [
    "#********** unique list of emojis found in corpus. ************\n",
    "print(uni_single_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let me make an emoji dictionary with these unique emojis from my dataset corpus.\n",
    "cnt = 1\n",
    "emoji_dict = {}\n",
    "reverse_lookup_emoji_dict={}\n",
    "\n",
    "for em in uni_single_emoji:\n",
    "    val = 'EMOJI_' +str(cnt)\n",
    "    emoji_dict[em] = val\n",
    "    reverse_lookup_emoji_dict[val]=em\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'❤': 'EMOJI_1', '☺': 'EMOJI_2', '👍': 'EMOJI_3', '😡': 'EMOJI_4', '😢': 'EMOJI_5', '💜': 'EMOJI_6', '✈': 'EMOJI_7', '🍷': 'EMOJI_8', '💺': 'EMOJI_9', '😊': 'EMOJI_10', '😍': 'EMOJI_11', '👌': 'EMOJI_12', '💕': 'EMOJI_13', '🌞': 'EMOJI_14', '😃': 'EMOJI_15', '😩': 'EMOJI_16', '😭': 'EMOJI_17', '😎': 'EMOJI_18', '🙉': 'EMOJI_19', '😁': 'EMOJI_20', '❄': 'EMOJI_21', '👏': 'EMOJI_22', '😂': 'EMOJI_23', '💗': 'EMOJI_24', '🍸': 'EMOJI_25', '😒': 'EMOJI_26', '👎': 'EMOJI_27', '😀': 'EMOJI_28', '😄': 'EMOJI_29', '😘': 'EMOJI_30', '👸': 'EMOJI_31', '🎀': 'EMOJI_32', '🌏': 'EMOJI_33', '😥': 'EMOJI_34', '😉': 'EMOJI_35', '✨': 'EMOJI_36', '😱': 'EMOJI_37', '🎉': 'EMOJI_38', '🙌': 'EMOJI_39', '💤': 'EMOJI_40', '😞': 'EMOJI_41', '♥': 'EMOJI_42', '👋': 'EMOJI_43', '✌': 'EMOJI_44', '🙏': 'EMOJI_45', '👿': 'EMOJI_46', '😔': 'EMOJI_47', '🙅': 'EMOJI_48', '🆖': 'EMOJI_49', '💩': 'EMOJI_50', '✔': 'EMOJI_51', '🌴': 'EMOJI_52', '✅': 'EMOJI_53', '❌': 'EMOJI_54', '👠': 'EMOJI_55', '😜': 'EMOJI_56', '😻': 'EMOJI_57', '😕': 'EMOJI_58', '😈': 'EMOJI_59', '😤': 'EMOJI_60', '💪': 'EMOJI_61', '😫': 'EMOJI_62', '💔': 'EMOJI_63', '😪': 'EMOJI_64', '😣': 'EMOJI_65', '😬': 'EMOJI_66', '💁': 'EMOJI_67', '😋': 'EMOJI_68', '😏': 'EMOJI_69', '😖': 'EMOJI_70', '🌟': 'EMOJI_71', '📱': 'EMOJI_72', '🍻': 'EMOJI_73', '💖': 'EMOJI_74', '😅': 'EMOJI_75', '💝': 'EMOJI_76', '🚫': 'EMOJI_77', '↔': 'EMOJI_78', '😷': 'EMOJI_79', '⭐': 'EMOJI_80', '🎵': 'EMOJI_81', '❗': 'EMOJI_82', '🐴': 'EMOJI_83', '😆': 'EMOJI_84', '😑': 'EMOJI_85', '🐩': 'EMOJI_86', '⤴': 'EMOJI_87', '☀': 'EMOJI_88', '👊': 'EMOJI_89', '💯': 'EMOJI_90', '😠': 'EMOJI_91', '☕': 'EMOJI_92', '📲': 'EMOJI_93', '👺': 'EMOJI_94', '🙈': 'EMOJI_95', '💘': 'EMOJI_96', '💙': 'EMOJI_97', '👉': 'EMOJI_98', '🚪': 'EMOJI_99', '😳': 'EMOJI_100', '😵': 'EMOJI_101', '🚶': 'EMOJI_102', '🔵': 'EMOJI_103', '😐': 'EMOJI_104', '👀': 'EMOJI_105', '🍅': 'EMOJI_106', '🆘': 'EMOJI_107', '⛄': 'EMOJI_108', '😓': 'EMOJI_109', '🎲': 'EMOJI_110', '⌚': 'EMOJI_111', '🐳': 'EMOJI_112', '⤵': 'EMOJI_113', '😮': 'EMOJI_114', '😲': 'EMOJI_115', '😦': 'EMOJI_116', '➡': 'EMOJI_117'}\n"
     ]
    }
   ],
   "source": [
    "print(emoji_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EMOJI_1': '❤', 'EMOJI_2': '☺', 'EMOJI_3': '👍', 'EMOJI_4': '😡', 'EMOJI_5': '😢', 'EMOJI_6': '💜', 'EMOJI_7': '✈', 'EMOJI_8': '🍷', 'EMOJI_9': '💺', 'EMOJI_10': '😊', 'EMOJI_11': '😍', 'EMOJI_12': '👌', 'EMOJI_13': '💕', 'EMOJI_14': '🌞', 'EMOJI_15': '😃', 'EMOJI_16': '😩', 'EMOJI_17': '😭', 'EMOJI_18': '😎', 'EMOJI_19': '🙉', 'EMOJI_20': '😁', 'EMOJI_21': '❄', 'EMOJI_22': '👏', 'EMOJI_23': '😂', 'EMOJI_24': '💗', 'EMOJI_25': '🍸', 'EMOJI_26': '😒', 'EMOJI_27': '👎', 'EMOJI_28': '😀', 'EMOJI_29': '😄', 'EMOJI_30': '😘', 'EMOJI_31': '👸', 'EMOJI_32': '🎀', 'EMOJI_33': '🌏', 'EMOJI_34': '😥', 'EMOJI_35': '😉', 'EMOJI_36': '✨', 'EMOJI_37': '😱', 'EMOJI_38': '🎉', 'EMOJI_39': '🙌', 'EMOJI_40': '💤', 'EMOJI_41': '😞', 'EMOJI_42': '♥', 'EMOJI_43': '👋', 'EMOJI_44': '✌', 'EMOJI_45': '🙏', 'EMOJI_46': '👿', 'EMOJI_47': '😔', 'EMOJI_48': '🙅', 'EMOJI_49': '🆖', 'EMOJI_50': '💩', 'EMOJI_51': '✔', 'EMOJI_52': '🌴', 'EMOJI_53': '✅', 'EMOJI_54': '❌', 'EMOJI_55': '👠', 'EMOJI_56': '😜', 'EMOJI_57': '😻', 'EMOJI_58': '😕', 'EMOJI_59': '😈', 'EMOJI_60': '😤', 'EMOJI_61': '💪', 'EMOJI_62': '😫', 'EMOJI_63': '💔', 'EMOJI_64': '😪', 'EMOJI_65': '😣', 'EMOJI_66': '😬', 'EMOJI_67': '💁', 'EMOJI_68': '😋', 'EMOJI_69': '😏', 'EMOJI_70': '😖', 'EMOJI_71': '🌟', 'EMOJI_72': '📱', 'EMOJI_73': '🍻', 'EMOJI_74': '💖', 'EMOJI_75': '😅', 'EMOJI_76': '💝', 'EMOJI_77': '🚫', 'EMOJI_78': '↔', 'EMOJI_79': '😷', 'EMOJI_80': '⭐', 'EMOJI_81': '🎵', 'EMOJI_82': '❗', 'EMOJI_83': '🐴', 'EMOJI_84': '😆', 'EMOJI_85': '😑', 'EMOJI_86': '🐩', 'EMOJI_87': '⤴', 'EMOJI_88': '☀', 'EMOJI_89': '👊', 'EMOJI_90': '💯', 'EMOJI_91': '😠', 'EMOJI_92': '☕', 'EMOJI_93': '📲', 'EMOJI_94': '👺', 'EMOJI_95': '🙈', 'EMOJI_96': '💘', 'EMOJI_97': '💙', 'EMOJI_98': '👉', 'EMOJI_99': '🚪', 'EMOJI_100': '😳', 'EMOJI_101': '😵', 'EMOJI_102': '🚶', 'EMOJI_103': '🔵', 'EMOJI_104': '😐', 'EMOJI_105': '👀', 'EMOJI_106': '🍅', 'EMOJI_107': '🆘', 'EMOJI_108': '⛄', 'EMOJI_109': '😓', 'EMOJI_110': '🎲', 'EMOJI_111': '⌚', 'EMOJI_112': '🐳', 'EMOJI_113': '⤵', 'EMOJI_114': '😮', 'EMOJI_115': '😲', 'EMOJI_116': '😦', 'EMOJI_117': '➡'}\n"
     ]
    }
   ],
   "source": [
    "print(reverse_lookup_emoji_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle the full comprehensive emoji dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = data_out_dir+'full_emoji_dict.obj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename,'wb') as handle:\n",
    "    pickle.dump(emoji_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
