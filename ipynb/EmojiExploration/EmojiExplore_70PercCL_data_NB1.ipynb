{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/GA_DSI/Projects/capstone\n"
     ]
    }
   ],
   "source": [
    "cd /home/jovyan/GA_DSI/Projects/capstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lib.general_utilities as gu\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report,precision_recall_curve,confusion_matrix \n",
    "from sklearn.metrics import (precision_score,accuracy_score,roc_auc_score,roc_curve, \n",
    "                             precision_recall_curve,recall_score,make_scorer,auc)\n",
    "from tqdm import tqdm\n",
    "\n",
    "# storing data directories for this notebook.\n",
    "img_out_dir = 'data/images/Emoticon_NB4/'\n",
    "data_out_dir = 'data/pickled/Emoticon_NB4/'\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled dataset.\n",
    "filename = 'data/airline_cl_process_full_dataset_70perc_conf_df'\n",
    "air_df = gu.read_pickle_obj(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>stopw_clean_text</th>\n",
       "      <th>stem_stopw_clean_text</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>AT_USER what AT_USER said.</td>\n",
       "      <td>said.</td>\n",
       "      <td>said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>AT_USER it's really aggressive to blast obnoxi...</td>\n",
       "      <td>it's really aggressive blast obnoxious \"entert...</td>\n",
       "      <td>it' realli aggress blast obnoxi \"entertainment...</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>AT_USER and it's a really big bad thing about it</td>\n",
       "      <td>it's really big bad thing</td>\n",
       "      <td>it' realli big bad thing</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>AT_USER seriously would pay $30 a flight for s...</td>\n",
       "      <td>seriously pay $30 seats didn't playing. it's r...</td>\n",
       "      <td>serious pay $30 seat didn't playing. it' reall...</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n",
       "      <td>AT_USER it was amazing, and arrived an hour ea...</td>\n",
       "      <td>amazing, arrived hour early. you're good me.</td>\n",
       "      <td>amazing, arriv hour early. you'r good me.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          airline airline_sentiment  \\\n",
       "0  Virgin America           neutral   \n",
       "1  Virgin America          negative   \n",
       "2  Virgin America          negative   \n",
       "3  Virgin America          negative   \n",
       "4  Virgin America          positive   \n",
       "\n",
       "                                                text  \\\n",
       "0                @VirginAmerica What @dhepburn said.   \n",
       "1  @VirginAmerica it's really aggressive to blast...   \n",
       "2  @VirginAmerica and it's a really big bad thing...   \n",
       "3  @VirginAmerica seriously would pay $30 a fligh...   \n",
       "4  @VirginAmerica it was amazing, and arrived an ...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0                         AT_USER what AT_USER said.   \n",
       "1  AT_USER it's really aggressive to blast obnoxi...   \n",
       "2   AT_USER and it's a really big bad thing about it   \n",
       "3  AT_USER seriously would pay $30 a flight for s...   \n",
       "4  AT_USER it was amazing, and arrived an hour ea...   \n",
       "\n",
       "                                    stopw_clean_text  \\\n",
       "0                                              said.   \n",
       "1  it's really aggressive blast obnoxious \"entert...   \n",
       "2                          it's really big bad thing   \n",
       "3  seriously pay $30 seats didn't playing. it's r...   \n",
       "4       amazing, arrived hour early. you're good me.   \n",
       "\n",
       "                               stem_stopw_clean_text negativereason  \\\n",
       "0                                              said.            NaN   \n",
       "1  it' realli aggress blast obnoxi \"entertainment...     Bad Flight   \n",
       "2                           it' realli big bad thing     Can't Tell   \n",
       "3  serious pay $30 seat didn't playing. it' reall...     Can't Tell   \n",
       "4          amazing, arriv hour early. you'r good me.            NaN   \n",
       "\n",
       "   airline_sentiment_confidence  \n",
       "0                           1.0  \n",
       "1                           1.0  \n",
       "2                           1.0  \n",
       "3                           1.0  \n",
       "4                           1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10768, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chances are emoticons are a strong predictor of 'positive' tweets. Let's look at the positive\n",
    "# tweets to search for emojis.\n",
    "# df_pos = air_df[air_df.airline_sentiment=='positive']\n",
    "# df_pos.clean_text[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emojis line 10, 73, 164, 165, 220, 298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_df = air_df.iloc[[10, 73, 164, 165, 220, 298]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>stopw_clean_text</th>\n",
       "      <th>stem_stopw_clean_text</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>positive</td>\n",
       "      <td>I â¤ï¸ flying @VirginAmerica. â˜ºï¸ğŸ‘</td>\n",
       "      <td>i â¤ï¸ flying AT_USER â˜ºï¸ğŸ‘</td>\n",
       "      <td>â¤ï¸ flying â˜ºï¸ğŸ‘</td>\n",
       "      <td>â¤ï¸ fli â˜ºï¸ğŸ‘</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica - amazing customer  service, ag...</td>\n",
       "      <td>AT_USER - amazing customer service, again! ğŸ’•ğŸ’• ...</td>\n",
       "      <td>- amazing customer service, again! ğŸ’•ğŸ’• raeann s...</td>\n",
       "      <td>- amaz custom service, again! ğŸ’•ğŸ’• raeann sf - s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica Very nicely done. ğŸ‘</td>\n",
       "      <td>AT_USER very nicely done. ğŸ‘</td>\n",
       "      <td>nicely done. ğŸ‘</td>\n",
       "      <td>nice done. ğŸ‘</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica hahaha ğŸ˜‚@VirginAmerica YOU GUYS...</td>\n",
       "      <td>AT_USER hahaha ğŸ˜‚AT_USER you guys are amazing. ...</td>\n",
       "      <td>hahaha ğŸ˜‚AT_USER guys amazing. love guys!!!ğŸ’—</td>\n",
       "      <td>hahaha ğŸ˜‚at_us guy amazing. love guys!!!ğŸ’—</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica thank you! I absolutely will ğŸ˜</td>\n",
       "      <td>AT_USER thank you! i absolutely will ğŸ˜</td>\n",
       "      <td>thank you! absolutely ğŸ˜</td>\n",
       "      <td>thank you! absolut ğŸ˜</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica Congrats VX on the new route! âœˆï¸ğŸ‰</td>\n",
       "      <td>AT_USER congrats vx on the new route! âœˆï¸ğŸ‰</td>\n",
       "      <td>congrats vx new route! âœˆï¸ğŸ‰</td>\n",
       "      <td>congrat vx new route! âœˆï¸ğŸ‰</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            airline airline_sentiment  \\\n",
       "10   Virgin America          positive   \n",
       "73   Virgin America          positive   \n",
       "164  Virgin America          positive   \n",
       "165  Virgin America          positive   \n",
       "220  Virgin America          positive   \n",
       "298  Virgin America          positive   \n",
       "\n",
       "                                                  text  \\\n",
       "10                     I â¤ï¸ flying @VirginAmerica. â˜ºï¸ğŸ‘   \n",
       "73   @VirginAmerica - amazing customer  service, ag...   \n",
       "164                 @VirginAmerica Very nicely done. ğŸ‘   \n",
       "165  @VirginAmerica hahaha ğŸ˜‚@VirginAmerica YOU GUYS...   \n",
       "220      @VirginAmerica thank you! I absolutely will ğŸ˜   \n",
       "298   @VirginAmerica Congrats VX on the new route! âœˆï¸ğŸ‰   \n",
       "\n",
       "                                            clean_text  \\\n",
       "10                             i â¤ï¸ flying AT_USER â˜ºï¸ğŸ‘   \n",
       "73   AT_USER - amazing customer service, again! ğŸ’•ğŸ’• ...   \n",
       "164                        AT_USER very nicely done. ğŸ‘   \n",
       "165  AT_USER hahaha ğŸ˜‚AT_USER you guys are amazing. ...   \n",
       "220             AT_USER thank you! i absolutely will ğŸ˜   \n",
       "298          AT_USER congrats vx on the new route! âœˆï¸ğŸ‰   \n",
       "\n",
       "                                      stopw_clean_text  \\\n",
       "10                                       â¤ï¸ flying â˜ºï¸ğŸ‘   \n",
       "73   - amazing customer service, again! ğŸ’•ğŸ’• raeann s...   \n",
       "164                                     nicely done. ğŸ‘   \n",
       "165        hahaha ğŸ˜‚AT_USER guys amazing. love guys!!!ğŸ’—   \n",
       "220                            thank you! absolutely ğŸ˜   \n",
       "298                         congrats vx new route! âœˆï¸ğŸ‰   \n",
       "\n",
       "                                 stem_stopw_clean_text negativereason  \\\n",
       "10                                          â¤ï¸ fli â˜ºï¸ğŸ‘            NaN   \n",
       "73   - amaz custom service, again! ğŸ’•ğŸ’• raeann sf - s...            NaN   \n",
       "164                                       nice done. ğŸ‘            NaN   \n",
       "165           hahaha ğŸ˜‚at_us guy amazing. love guys!!!ğŸ’—            NaN   \n",
       "220                               thank you! absolut ğŸ˜            NaN   \n",
       "298                          congrat vx new route! âœˆï¸ğŸ‰            NaN   \n",
       "\n",
       "     airline_sentiment_confidence  \n",
       "10                            1.0  \n",
       "73                            1.0  \n",
       "164                           1.0  \n",
       "165                           1.0  \n",
       "220                           1.0  \n",
       "298                           1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what countvectorizer does with the emojis.\n",
    "cv = CountVectorizer()\n",
    "corpus = emoji_df.clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10                               i â¤ï¸ flying AT_USER â˜ºï¸ğŸ‘\n",
       "73     AT_USER - amazing customer service, again! ğŸ’•ğŸ’• ...\n",
       "164                          AT_USER very nicely done. ğŸ‘\n",
       "165    AT_USER hahaha ğŸ˜‚AT_USER you guys are amazing. ...\n",
       "220               AT_USER thank you! i absolutely will ğŸ˜\n",
       "298            AT_USER congrats vx on the new route! âœˆï¸ğŸ‰\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absolutely', 'again', 'amazing', 'are', 'at_user', 'best', 'congrats', 'customer', 'customerservice', 'done', 'flying', 'guys', 'hahaha', 'in', 'love', 'new', 'nicely', 'on', 'raeann', 'route', 'service', 'sf', 'she', 'thank', 'the', 'very', 'virginamerica', 'vx', 'will', 'you']\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check Tfdf.\n",
    "tf = TfidfVectorizer()\n",
    "tf.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absolutely', 'again', 'amazing', 'are', 'at_user', 'best', 'congrats', 'customer', 'customerservice', 'done', 'flying', 'guys', 'hahaha', 'in', 'love', 'new', 'nicely', 'on', 'raeann', 'route', 'service', 'sf', 'she', 'thank', 'the', 'very', 'virginamerica', 'vx', 'will', 'you']\n"
     ]
    }
   ],
   "source": [
    "print(tf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No emojis. We probably need to encode them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_tok = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to tokenize the corpus above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokens=[]\n",
    "for text in corpus:\n",
    "    tweet_tokens+=tw_tok.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'â¤', 'ï¸', 'flying', 'AT_USER', 'â˜º', 'ï¸', 'ğŸ‘', 'AT_USER', '-', 'amazing', 'customer', 'service', ',', 'again', '!', 'ğŸ’•', 'ğŸ’•', 'raeann', 'in', 'sf', '-', \"she's\", 'the', 'best', '!', 'customerservice', 'virginamerica', 'flying', 'AT_USER', 'very', 'nicely', 'done', '.', 'ğŸ‘', 'AT_USER', 'hahaha', 'ğŸ˜‚', 'AT_USER', 'you', 'guys', 'are', 'amazing', '.', 'i', 'love', 'you', 'guys', '!', '!', '!', 'ğŸ’—', 'AT_USER', 'thank', 'you', '!', 'i', 'absolutely', 'will', 'ğŸ˜', 'AT_USER', 'congrats', 'vx', 'on', 'the', 'new', 'route', '!', 'âœˆ', 'ï¸', 'ğŸ‰']\n"
     ]
    }
   ],
   "source": [
    "print(tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10                               i â¤ï¸ flying AT_USER â˜ºï¸ğŸ‘\n",
       "73     AT_USER - amazing customer service, again! ğŸ’•ğŸ’• ...\n",
       "164                          AT_USER very nicely done. ğŸ‘\n",
       "165    AT_USER hahaha ğŸ˜‚AT_USER you guys are amazing. ...\n",
       "220               AT_USER thank you! i absolutely will ğŸ˜\n",
       "298            AT_USER congrats vx on the new route! âœˆï¸ğŸ‰\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for text in corpus:\n",
    "    a+=text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'â¤ï¸', 'flying', 'AT_USER', 'â˜ºï¸ğŸ‘', 'AT_USER', '-', 'amazing', 'customer', 'service,', 'again!', 'ğŸ’•ğŸ’•', 'raeann', 'in', 'sf', '-', \"she's\", 'the', 'best!', 'customerservice', 'virginamerica', 'flying', 'AT_USER', 'very', 'nicely', 'done.', 'ğŸ‘', 'AT_USER', 'hahaha', 'ğŸ˜‚AT_USER', 'you', 'guys', 'are', 'amazing.', 'i', 'love', 'you', 'guys!!!ğŸ’—', 'AT_USER', 'thank', 'you!', 'i', 'absolutely', 'will', 'ğŸ˜', 'AT_USER', 'congrats', 'vx', 'on', 'the', 'new', 'route!', 'âœˆï¸ğŸ‰']\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /opt/conda/lib/python3.6/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emojis(text):\n",
    "    return ' '.join(c for c in text if c in emoji.UNICODE_EMOJI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i â¤ï¸ flying AT_USER â˜ºï¸ğŸ‘'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = corpus.values[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'â¤ â˜º ğŸ‘'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_emojis(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AT_USER - amazing customer service, again! ğŸ’•ğŸ’• raeann in sf - she's the best! customerservice virginamerica flying\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = corpus.values[1]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ğŸ’• ğŸ’•'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_emojis(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***** Create a column in our data to store only emojis found in the text of the tweet. *****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column of only emojis for that text.\n",
    "air_df['emojis'] = air_df['text'].apply(extract_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_col = air_df.emojis[air_df.emojis!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['â¤ â˜º ğŸ‘', 'ğŸ˜¡', 'ğŸ˜¢', 'ğŸ’œ âœˆ', 'ğŸ· ğŸ‘ ğŸ’º âœˆ', 'ğŸ’• ğŸ’•', 'ğŸ˜', 'â¤', 'ğŸ‘', 'ğŸ˜‚ ğŸ’—'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_col.values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the subset of data that contain emojis.\n",
    "df_emoji = air_df.iloc[em_col.index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emojis</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I â¤ï¸ flying @VirginAmerica. â˜ºï¸ğŸ‘</td>\n",
       "      <td>â¤ â˜º ğŸ‘</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@VirginAmerica you guys messed up my seating.....</td>\n",
       "      <td>ğŸ˜¡</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@VirginAmerica hi! I just bked a cool birthday...</td>\n",
       "      <td>ğŸ˜¢</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>@VirginAmerica Moodlighting is the only way to...</td>\n",
       "      <td>ğŸ’œ âœˆ</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>@VirginAmerica plz help me win my bid upgrade ...</td>\n",
       "      <td>ğŸ· ğŸ‘ ğŸ’º âœˆ</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>@VirginAmerica - amazing customer  service, ag...</td>\n",
       "      <td>ğŸ’• ğŸ’•</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>@VirginAmerica trying to book a flight &amp;amp; y...</td>\n",
       "      <td>ğŸ˜</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>@VirginAmerica my goodness your people @love f...</td>\n",
       "      <td>â¤</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>@VirginAmerica Very nicely done. ğŸ‘</td>\n",
       "      <td>ğŸ‘</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>@VirginAmerica hahaha ğŸ˜‚@VirginAmerica YOU GUYS...</td>\n",
       "      <td>ğŸ˜‚ ğŸ’—</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text   emojis  \\\n",
       "10                     I â¤ï¸ flying @VirginAmerica. â˜ºï¸ğŸ‘    â¤ â˜º ğŸ‘   \n",
       "15   @VirginAmerica you guys messed up my seating.....        ğŸ˜¡   \n",
       "19   @VirginAmerica hi! I just bked a cool birthday...        ğŸ˜¢   \n",
       "24   @VirginAmerica Moodlighting is the only way to...      ğŸ’œ âœˆ   \n",
       "27   @VirginAmerica plz help me win my bid upgrade ...  ğŸ· ğŸ‘ ğŸ’º âœˆ   \n",
       "73   @VirginAmerica - amazing customer  service, ag...      ğŸ’• ğŸ’•   \n",
       "138  @VirginAmerica trying to book a flight &amp; y...        ğŸ˜   \n",
       "146  @VirginAmerica my goodness your people @love f...        â¤   \n",
       "164                 @VirginAmerica Very nicely done. ğŸ‘        ğŸ‘   \n",
       "165  @VirginAmerica hahaha ğŸ˜‚@VirginAmerica YOU GUYS...      ğŸ˜‚ ğŸ’—   \n",
       "\n",
       "    airline_sentiment  \n",
       "10           positive  \n",
       "15           negative  \n",
       "19           negative  \n",
       "24           positive  \n",
       "27            neutral  \n",
       "73           positive  \n",
       "138          negative  \n",
       "146          positive  \n",
       "164          positive  \n",
       "165          positive  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emoji[['text', 'emojis', 'airline_sentiment']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emojis</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I â¤ï¸ flying @VirginAmerica. â˜ºï¸ğŸ‘</td>\n",
       "      <td>â¤ â˜º ğŸ‘</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@VirginAmerica you guys messed up my seating.....</td>\n",
       "      <td>ğŸ˜¡</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@VirginAmerica hi! I just bked a cool birthday...</td>\n",
       "      <td>ğŸ˜¢</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>@VirginAmerica Moodlighting is the only way to...</td>\n",
       "      <td>ğŸ’œ âœˆ</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text emojis airline_sentiment\n",
       "10                    I â¤ï¸ flying @VirginAmerica. â˜ºï¸ğŸ‘  â¤ â˜º ğŸ‘          positive\n",
       "15  @VirginAmerica you guys messed up my seating.....      ğŸ˜¡          negative\n",
       "19  @VirginAmerica hi! I just bked a cool birthday...      ğŸ˜¢          negative\n",
       "24  @VirginAmerica Moodlighting is the only way to...    ğŸ’œ âœˆ          positive"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emoji.loc[[10,15,19,24], ['text','emojis', 'airline_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list on unique emojis and the class they appear in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['â¤ â˜º ğŸ‘', 'ğŸ˜¡', 'ğŸ˜¢', 'ğŸ’œ âœˆ', 'ğŸ· ğŸ‘ ğŸ’º âœˆ', 'ğŸ’• ğŸ’•', 'ğŸ˜', 'â¤', 'ğŸ‘', 'ğŸ˜‚ ğŸ’—',\n",
       "       'ğŸ¸', 'ğŸ˜’', 'ğŸ‘', 'ğŸ‘ ğŸ‘ âœˆ âœˆ ğŸ’—', 'ğŸ˜Š ğŸ˜€ ğŸ˜ƒ ğŸ˜„', 'ğŸ˜', 'ğŸ‘¸ ğŸ’—', 'ğŸ˜¥', 'ğŸ€ ğŸ€ ğŸ€',\n",
       "       'âœˆ ğŸ‰', 'ğŸ’— ğŸ€ ğŸ’—', 'ğŸ˜ƒ ğŸ‘', 'ğŸ‘‹', 'âœŒ', 'ğŸ™', 'ğŸ’œ', 'ğŸ‘¿', 'ğŸ˜‰ ğŸ˜‰', 'ğŸ˜”', 'ğŸ˜­',\n",
       "       'ğŸ˜Š', 'âœˆ', 'ğŸ˜¡ ğŸ˜¡', 'ğŸ‘ ğŸ‘ ğŸ‘ âœˆ', 'ğŸ‘', 'ğŸ†–', 'ğŸ’©', 'âœ”', 'ğŸŒ´ ğŸŒ´', 'âœ… âŒ',\n",
       "       'ğŸ‘ ğŸ‘ ğŸ‘ ğŸ‘ ğŸ‘ ğŸ‘', 'ğŸ˜„ ğŸ˜„ ğŸ˜„ ğŸ˜¡ ğŸ˜¡ ğŸ˜¡', 'ğŸ‘ ğŸ‘', 'ğŸ˜', 'ğŸ‰ ğŸ‰ ğŸ‰', 'ğŸ˜‰', 'ğŸ‘ ğŸ‘', 'ğŸ˜ˆ',\n",
       "       'ğŸ˜¡ ğŸ˜¡ ğŸ˜¡ ğŸ˜¡ ğŸ˜¤ ğŸ˜¤ ğŸ˜¤', 'ğŸ‘ ğŸ‘ ğŸ‘', 'ğŸ‘ ğŸ˜Š', 'ğŸ‘ ğŸ‘Œ', 'ğŸ˜‚', 'ğŸ˜€', 'ğŸ‘Œ', 'ğŸ’ª', 'ğŸ’” ğŸ˜ª',\n",
       "       'ğŸ˜•', 'ğŸ˜£', 'ğŸ˜¬', 'ğŸ˜„', 'ğŸ˜‹', 'ğŸ™Œ ğŸ˜', 'ğŸŒŸ ğŸŒŸ', 'âœˆ ğŸ“±', 'ğŸ‘ ğŸ‘ ğŸ» ğŸ»', 'ğŸ˜ ğŸ˜¡', 'ğŸ’–',\n",
       "       'ğŸ˜” ğŸ˜” ğŸ˜”', 'ğŸ’', 'ğŸ˜', 'ğŸ˜œ ğŸ˜‚', 'ğŸ˜· ğŸ˜±', 'ğŸ˜–', 'â­ âœˆ', 'ğŸ˜ƒ ğŸ’• ğŸµ âœˆ â— â¤', 'ğŸ˜¤ ğŸ´',\n",
       "       'ğŸ˜­ ğŸ˜­ ğŸ˜­', 'ğŸ˜­ ğŸ˜­ ğŸ’” ğŸ’” ğŸ’” ğŸ’” ğŸ’” ğŸ’” ğŸ’”', 'ğŸ˜†', 'ğŸ˜Š ğŸŒ´', 'ğŸ‘ ğŸ‘', 'âœˆ âœˆ', 'âœˆ ğŸ˜ƒ ğŸ‘',\n",
       "       'â¤ â¤ â¤', 'ğŸ˜©', 'ğŸ˜‘', 'ğŸ’•', 'ğŸ˜ƒ ğŸ’• ğŸ˜ â¤´ â¤´', 'ğŸ˜’ ğŸ‘', 'ğŸ˜œ', 'â˜€', 'â¤ ğŸ‘Š', 'ğŸ˜ƒ',\n",
       "       'ğŸ˜­ ğŸ˜­', 'ğŸ’¯', 'ğŸ’© ğŸ’© ğŸ’© ğŸ’©', 'ğŸ˜  ğŸ˜ ', 'ğŸ˜Š â˜• ğŸ“² âœˆ', 'ğŸ˜ ', 'ğŸ‘º', 'ğŸ™ˆ', 'ğŸ’˜',\n",
       "       'ğŸ‘ ğŸ‘ ğŸ‘ ğŸ‘', 'ğŸ‘‰ ğŸšª', 'ğŸ™…', 'ğŸ˜‚ ğŸ˜‚', 'ğŸ˜­ ğŸ˜ ğŸ˜† ğŸ˜µ', 'âœˆ ğŸ”µ ğŸ”µ ğŸ”µ', 'ğŸ’™', 'ğŸ‘€ ğŸ‘€',\n",
       "       'ğŸ˜‚ ğŸ˜‚ ğŸ˜­ ğŸ˜­ ğŸ˜­ ğŸ˜­', 'ğŸ‘€', 'ğŸ˜', 'ğŸ˜Š ğŸ˜Š', 'ğŸ’™ ğŸ’™ ğŸ’™ ğŸ’™', 'ğŸ˜‚ ğŸ‘Œ ğŸ‘Œ ğŸ‘Œ', 'ğŸ˜ ğŸ˜ ğŸ˜', 'â˜º',\n",
       "       'ğŸ™Œ âœˆ', 'ğŸŒ´', 'â˜º ğŸ‘ ğŸ‘', 'ğŸ˜ ğŸ‰', 'ğŸ†˜ ğŸ†˜ ğŸ†˜ ğŸ†˜ ğŸ†˜ ğŸ†˜ ğŸ†˜ ğŸ†˜ ğŸ†˜ ğŸ†˜ ğŸ†˜ ğŸ†˜ ğŸ†˜ ğŸ†˜', 'ğŸ˜˜',\n",
       "       'ğŸ‘ ğŸ‘ ğŸ˜Š', 'ğŸ’”', 'ğŸ˜·', 'ğŸ’™ ğŸ’™', 'ğŸ˜Š âœˆ', 'â¤ âœ¨', 'â˜• âœˆ ğŸ‘', 'ğŸ˜¢ ğŸ˜¢', 'ğŸ·',\n",
       "       'ğŸ˜Š ğŸ˜Š ğŸ˜Š âœˆ âœˆ âœˆ', 'ğŸ‘Œ â˜º', 'ğŸ’• âœˆ ğŸ’º', 'ğŸ˜” ğŸ˜“ ğŸ˜¤', 'ğŸ‘ ğŸ‘', 'ğŸ™ ğŸ™ ğŸ™', 'ğŸ˜¡ ğŸ˜¡ ğŸ˜¡',\n",
       "       'âŒš', 'ğŸ˜‚ ğŸ˜‚ ğŸ˜‚', 'ğŸ‘Š', 'ğŸ˜³', 'ğŸ˜‘ ğŸ˜©', 'ğŸ³', 'ğŸ’º âœˆ', 'ğŸ™ ğŸ™ ğŸ™ âœŒ âœŒ âœŒ ğŸ™ ğŸ™ ğŸ™', 'â¤µ',\n",
       "       'ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™', 'ğŸ‘ âœˆ ğŸ‘ ğŸ˜¬ ğŸ‘ ğŸ˜¡ ğŸ˜Š ğŸ‘', 'ğŸ˜¤',\n",
       "       'ğŸ‘  ğŸ‘  ğŸ‘ ',\n",
       "       'ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ˜¢ ğŸ˜¢ ğŸ˜¢ ğŸ˜¢ ğŸ˜¢ ğŸ˜¢ ğŸ˜¢ ğŸ˜¢ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™',\n",
       "       'ğŸ™ ğŸ™ ğŸ™ ğŸ˜¢ ğŸ˜¢ ğŸ˜¢ ğŸ™ ğŸ™ ğŸ™',\n",
       "       'ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™ ğŸ™',\n",
       "       'ğŸ™ ğŸ™ ğŸ™ âœŒ ğŸ˜‰', 'âœˆ ğŸŒ', 'ğŸ‘ ğŸ˜¡ âœˆ', 'ğŸ‘Œ ğŸ‘Œ ğŸ‘Œ', 'ğŸ‘ ğŸ‘ ğŸ‘ ğŸ‘ ğŸ‘', 'ğŸ˜¥ ğŸ˜¥', 'ğŸ˜„ ğŸ’ ğŸ’ ğŸ’',\n",
       "       'ğŸ˜¡ ğŸ˜¤ ğŸ˜– ğŸ˜² ğŸ˜©', 'ğŸ˜¢ ğŸ˜• ğŸ˜¦', 'â¤ â¤', 'ğŸ˜“ ğŸ˜­', 'â¡ ğŸ˜•', 'âœˆ âœˆ âœˆ âœˆ âœˆ âœˆ âœˆ', 'âœˆ âœŒ'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_emoji_strings = df_emoji.emojis.unique()\n",
    "unique_emoji_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(unique_emoji_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['â¤', 'â˜º', 'ğŸ‘']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emoji.emojis.values[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***** Unique list of emoji features. *****\n",
    "uni_emoji_feat_lst = unique_emoji_strings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['â¤ â˜º ğŸ‘', 'ğŸ˜¡', 'ğŸ˜¢', 'ğŸ’œ âœˆ', 'ğŸ· ğŸ‘ ğŸ’º âœˆ', 'ğŸ’• ğŸ’•', 'ğŸ˜', 'â¤', 'ğŸ‘', 'ğŸ˜‚ ğŸ’—']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_emoji_feat_lst[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uni_emoji_feat_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336, 9)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emoji.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['â¤', 'â˜º', 'ğŸ‘']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_emoji_feat_lst[0].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a list of unique single emojis found in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_single_emoji=[]\n",
    "for emoj_str in uni_emoji_feat_lst:\n",
    "    emoj_lst = emoj_str.split()\n",
    "    [uni_single_emoji.append(em) for em in emoj_lst if (em not in uni_single_emoji)] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uni_single_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â¤', 'â˜º', 'ğŸ‘', 'ğŸ˜¡', 'ğŸ˜¢', 'ğŸ’œ', 'âœˆ', 'ğŸ·', 'ğŸ’º', 'ğŸ’•', 'ğŸ˜', 'ğŸ‘', 'ğŸ˜‚', 'ğŸ’—', 'ğŸ¸', 'ğŸ˜’', 'ğŸ‘', 'ğŸ˜Š', 'ğŸ˜€', 'ğŸ˜ƒ', 'ğŸ˜„', 'ğŸ˜', 'ğŸ‘¸', 'ğŸ˜¥', 'ğŸ€', 'ğŸ‰', 'ğŸ‘‹', 'âœŒ', 'ğŸ™', 'ğŸ‘¿', 'ğŸ˜‰', 'ğŸ˜”', 'ğŸ˜­', 'ğŸ†–', 'ğŸ’©', 'âœ”', 'ğŸŒ´', 'âœ…', 'âŒ', 'ğŸ˜', 'ğŸ˜ˆ', 'ğŸ˜¤', 'ğŸ‘Œ', 'ğŸ’ª', 'ğŸ’”', 'ğŸ˜ª', 'ğŸ˜•', 'ğŸ˜£', 'ğŸ˜¬', 'ğŸ˜‹', 'ğŸ™Œ', 'ğŸ˜', 'ğŸŒŸ', 'ğŸ“±', 'ğŸ»', 'ğŸ’–', 'ğŸ’', 'ğŸ˜œ', 'ğŸ˜·', 'ğŸ˜±', 'ğŸ˜–', 'â­', 'ğŸµ', 'â—', 'ğŸ´', 'ğŸ˜†', 'ğŸ˜©', 'ğŸ˜‘', 'ğŸ˜', 'â¤´', 'â˜€', 'ğŸ‘Š', 'ğŸ’¯', 'ğŸ˜ ', 'â˜•', 'ğŸ“²', 'ğŸ‘º', 'ğŸ™ˆ', 'ğŸ’˜', 'ğŸ‘‰', 'ğŸšª', 'ğŸ™…', 'ğŸ˜µ', 'ğŸ”µ', 'ğŸ’™', 'ğŸ‘€', 'ğŸ˜', 'ğŸ†˜', 'ğŸ˜˜', 'âœ¨', 'ğŸ˜“', 'âŒš', 'ğŸ˜³', 'ğŸ³', 'â¤µ', 'ğŸ‘ ', 'ğŸŒ', 'ğŸ˜²', 'ğŸ˜¦', 'â¡']\n"
     ]
    }
   ],
   "source": [
    "#********** unique list of emojis found in corpus. ************\n",
    "print(uni_single_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_emojis = [em.encode('unicode-escape') for em in uni_single_emoji]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'\\\\u2764', b'\\\\u263a', b'\\\\U0001f44d', b'\\\\U0001f621', b'\\\\U0001f622', b'\\\\U0001f49c', b'\\\\u2708', b'\\\\U0001f377', b'\\\\U0001f4ba', b'\\\\U0001f495', b'\\\\U0001f601', b'\\\\U0001f44f', b'\\\\U0001f602', b'\\\\U0001f497', b'\\\\U0001f378', b'\\\\U0001f612', b'\\\\U0001f44e', b'\\\\U0001f60a', b'\\\\U0001f600', b'\\\\U0001f603', b'\\\\U0001f604', b'\\\\U0001f60e', b'\\\\U0001f478', b'\\\\U0001f625', b'\\\\U0001f380', b'\\\\U0001f389', b'\\\\U0001f44b', b'\\\\u270c', b'\\\\U0001f64f', b'\\\\U0001f47f', b'\\\\U0001f609', b'\\\\U0001f614', b'\\\\U0001f62d', b'\\\\U0001f196', b'\\\\U0001f4a9', b'\\\\u2714', b'\\\\U0001f334', b'\\\\u2705', b'\\\\u274c', b'\\\\U0001f61e', b'\\\\U0001f608', b'\\\\U0001f624', b'\\\\U0001f44c', b'\\\\U0001f4aa', b'\\\\U0001f494', b'\\\\U0001f62a', b'\\\\U0001f615', b'\\\\U0001f623', b'\\\\U0001f62c', b'\\\\U0001f60b', b'\\\\U0001f64c', b'\\\\U0001f60f', b'\\\\U0001f31f', b'\\\\U0001f4f1', b'\\\\U0001f37b', b'\\\\U0001f496', b'\\\\U0001f49d', b'\\\\U0001f61c', b'\\\\U0001f637', b'\\\\U0001f631', b'\\\\U0001f616', b'\\\\u2b50', b'\\\\U0001f3b5', b'\\\\u2757', b'\\\\U0001f434', b'\\\\U0001f606', b'\\\\U0001f629', b'\\\\U0001f611', b'\\\\U0001f60d', b'\\\\u2934', b'\\\\u2600', b'\\\\U0001f44a', b'\\\\U0001f4af', b'\\\\U0001f620', b'\\\\u2615', b'\\\\U0001f4f2', b'\\\\U0001f47a', b'\\\\U0001f648', b'\\\\U0001f498', b'\\\\U0001f449', b'\\\\U0001f6aa', b'\\\\U0001f645', b'\\\\U0001f635', b'\\\\U0001f535', b'\\\\U0001f499', b'\\\\U0001f440', b'\\\\U0001f610', b'\\\\U0001f198', b'\\\\U0001f618', b'\\\\u2728', b'\\\\U0001f613', b'\\\\u231a', b'\\\\U0001f633', b'\\\\U0001f433', b'\\\\u2935', b'\\\\U0001f460', b'\\\\U0001f31e', b'\\\\U0001f632', b'\\\\U0001f626', b'\\\\u27a1']\n"
     ]
    }
   ],
   "source": [
    "print(encoded_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â¤', 'â˜º', 'ğŸ‘', 'ğŸ˜¡', 'ğŸ˜¢', 'ğŸ’œ', 'âœˆ', 'ğŸ·', 'ğŸ’º', 'ğŸ’•', 'ğŸ˜', 'ğŸ‘', 'ğŸ˜‚', 'ğŸ’—', 'ğŸ¸', 'ğŸ˜’', 'ğŸ‘', 'ğŸ˜Š', 'ğŸ˜€', 'ğŸ˜ƒ', 'ğŸ˜„', 'ğŸ˜', 'ğŸ‘¸', 'ğŸ˜¥', 'ğŸ€', 'ğŸ‰', 'ğŸ‘‹', 'âœŒ', 'ğŸ™', 'ğŸ‘¿', 'ğŸ˜‰', 'ğŸ˜”', 'ğŸ˜­', 'ğŸ†–', 'ğŸ’©', 'âœ”', 'ğŸŒ´', 'âœ…', 'âŒ', 'ğŸ˜', 'ğŸ˜ˆ', 'ğŸ˜¤', 'ğŸ‘Œ', 'ğŸ’ª', 'ğŸ’”', 'ğŸ˜ª', 'ğŸ˜•', 'ğŸ˜£', 'ğŸ˜¬', 'ğŸ˜‹', 'ğŸ™Œ', 'ğŸ˜', 'ğŸŒŸ', 'ğŸ“±', 'ğŸ»', 'ğŸ’–', 'ğŸ’', 'ğŸ˜œ', 'ğŸ˜·', 'ğŸ˜±', 'ğŸ˜–', 'â­', 'ğŸµ', 'â—', 'ğŸ´', 'ğŸ˜†', 'ğŸ˜©', 'ğŸ˜‘', 'ğŸ˜', 'â¤´', 'â˜€', 'ğŸ‘Š', 'ğŸ’¯', 'ğŸ˜ ', 'â˜•', 'ğŸ“²', 'ğŸ‘º', 'ğŸ™ˆ', 'ğŸ’˜', 'ğŸ‘‰', 'ğŸšª', 'ğŸ™…', 'ğŸ˜µ', 'ğŸ”µ', 'ğŸ’™', 'ğŸ‘€', 'ğŸ˜', 'ğŸ†˜', 'ğŸ˜˜', 'âœ¨', 'ğŸ˜“', 'âŒš', 'ğŸ˜³', 'ğŸ³', 'â¤µ', 'ğŸ‘ ', 'ğŸŒ', 'ğŸ˜²', 'ğŸ˜¦', 'â¡']\n"
     ]
    }
   ],
   "source": [
    "decoded_emojis = [em.decode('unicode-escape') for em in encoded_emojis]\n",
    "print(decoded_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let me make an emoji dictionary with these unique emojis from my dataset corpus.\n",
    "cnt = 1\n",
    "emoji_dict = {}\n",
    "reverse_lookup_emoji_dict={}\n",
    "\n",
    "for em in uni_single_emoji:\n",
    "    val = 'EMOJI_' +str(cnt)\n",
    "    emoji_dict[em] = val\n",
    "    reverse_lookup_emoji_dict[val]=em\n",
    "    cnt+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'â¤': 'EMOJI_1', 'â˜º': 'EMOJI_2', 'ğŸ‘': 'EMOJI_3', 'ğŸ˜¡': 'EMOJI_4', 'ğŸ˜¢': 'EMOJI_5', 'ğŸ’œ': 'EMOJI_6', 'âœˆ': 'EMOJI_7', 'ğŸ·': 'EMOJI_8', 'ğŸ’º': 'EMOJI_9', 'ğŸ’•': 'EMOJI_10', 'ğŸ˜': 'EMOJI_11', 'ğŸ‘': 'EMOJI_12', 'ğŸ˜‚': 'EMOJI_13', 'ğŸ’—': 'EMOJI_14', 'ğŸ¸': 'EMOJI_15', 'ğŸ˜’': 'EMOJI_16', 'ğŸ‘': 'EMOJI_17', 'ğŸ˜Š': 'EMOJI_18', 'ğŸ˜€': 'EMOJI_19', 'ğŸ˜ƒ': 'EMOJI_20', 'ğŸ˜„': 'EMOJI_21', 'ğŸ˜': 'EMOJI_22', 'ğŸ‘¸': 'EMOJI_23', 'ğŸ˜¥': 'EMOJI_24', 'ğŸ€': 'EMOJI_25', 'ğŸ‰': 'EMOJI_26', 'ğŸ‘‹': 'EMOJI_27', 'âœŒ': 'EMOJI_28', 'ğŸ™': 'EMOJI_29', 'ğŸ‘¿': 'EMOJI_30', 'ğŸ˜‰': 'EMOJI_31', 'ğŸ˜”': 'EMOJI_32', 'ğŸ˜­': 'EMOJI_33', 'ğŸ†–': 'EMOJI_34', 'ğŸ’©': 'EMOJI_35', 'âœ”': 'EMOJI_36', 'ğŸŒ´': 'EMOJI_37', 'âœ…': 'EMOJI_38', 'âŒ': 'EMOJI_39', 'ğŸ˜': 'EMOJI_40', 'ğŸ˜ˆ': 'EMOJI_41', 'ğŸ˜¤': 'EMOJI_42', 'ğŸ‘Œ': 'EMOJI_43', 'ğŸ’ª': 'EMOJI_44', 'ğŸ’”': 'EMOJI_45', 'ğŸ˜ª': 'EMOJI_46', 'ğŸ˜•': 'EMOJI_47', 'ğŸ˜£': 'EMOJI_48', 'ğŸ˜¬': 'EMOJI_49', 'ğŸ˜‹': 'EMOJI_50', 'ğŸ™Œ': 'EMOJI_51', 'ğŸ˜': 'EMOJI_52', 'ğŸŒŸ': 'EMOJI_53', 'ğŸ“±': 'EMOJI_54', 'ğŸ»': 'EMOJI_55', 'ğŸ’–': 'EMOJI_56', 'ğŸ’': 'EMOJI_57', 'ğŸ˜œ': 'EMOJI_58', 'ğŸ˜·': 'EMOJI_59', 'ğŸ˜±': 'EMOJI_60', 'ğŸ˜–': 'EMOJI_61', 'â­': 'EMOJI_62', 'ğŸµ': 'EMOJI_63', 'â—': 'EMOJI_64', 'ğŸ´': 'EMOJI_65', 'ğŸ˜†': 'EMOJI_66', 'ğŸ˜©': 'EMOJI_67', 'ğŸ˜‘': 'EMOJI_68', 'ğŸ˜': 'EMOJI_69', 'â¤´': 'EMOJI_70', 'â˜€': 'EMOJI_71', 'ğŸ‘Š': 'EMOJI_72', 'ğŸ’¯': 'EMOJI_73', 'ğŸ˜ ': 'EMOJI_74', 'â˜•': 'EMOJI_75', 'ğŸ“²': 'EMOJI_76', 'ğŸ‘º': 'EMOJI_77', 'ğŸ™ˆ': 'EMOJI_78', 'ğŸ’˜': 'EMOJI_79', 'ğŸ‘‰': 'EMOJI_80', 'ğŸšª': 'EMOJI_81', 'ğŸ™…': 'EMOJI_82', 'ğŸ˜µ': 'EMOJI_83', 'ğŸ”µ': 'EMOJI_84', 'ğŸ’™': 'EMOJI_85', 'ğŸ‘€': 'EMOJI_86', 'ğŸ˜': 'EMOJI_87', 'ğŸ†˜': 'EMOJI_88', 'ğŸ˜˜': 'EMOJI_89', 'âœ¨': 'EMOJI_90', 'ğŸ˜“': 'EMOJI_91', 'âŒš': 'EMOJI_92', 'ğŸ˜³': 'EMOJI_93', 'ğŸ³': 'EMOJI_94', 'â¤µ': 'EMOJI_95', 'ğŸ‘ ': 'EMOJI_96', 'ğŸŒ': 'EMOJI_97', 'ğŸ˜²': 'EMOJI_98', 'ğŸ˜¦': 'EMOJI_99', 'â¡': 'EMOJI_100'}\n"
     ]
    }
   ],
   "source": [
    "print(emoji_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EMOJI_1': 'â¤', 'EMOJI_2': 'â˜º', 'EMOJI_3': 'ğŸ‘', 'EMOJI_4': 'ğŸ˜¡', 'EMOJI_5': 'ğŸ˜¢', 'EMOJI_6': 'ğŸ’œ', 'EMOJI_7': 'âœˆ', 'EMOJI_8': 'ğŸ·', 'EMOJI_9': 'ğŸ’º', 'EMOJI_10': 'ğŸ’•', 'EMOJI_11': 'ğŸ˜', 'EMOJI_12': 'ğŸ‘', 'EMOJI_13': 'ğŸ˜‚', 'EMOJI_14': 'ğŸ’—', 'EMOJI_15': 'ğŸ¸', 'EMOJI_16': 'ğŸ˜’', 'EMOJI_17': 'ğŸ‘', 'EMOJI_18': 'ğŸ˜Š', 'EMOJI_19': 'ğŸ˜€', 'EMOJI_20': 'ğŸ˜ƒ', 'EMOJI_21': 'ğŸ˜„', 'EMOJI_22': 'ğŸ˜', 'EMOJI_23': 'ğŸ‘¸', 'EMOJI_24': 'ğŸ˜¥', 'EMOJI_25': 'ğŸ€', 'EMOJI_26': 'ğŸ‰', 'EMOJI_27': 'ğŸ‘‹', 'EMOJI_28': 'âœŒ', 'EMOJI_29': 'ğŸ™', 'EMOJI_30': 'ğŸ‘¿', 'EMOJI_31': 'ğŸ˜‰', 'EMOJI_32': 'ğŸ˜”', 'EMOJI_33': 'ğŸ˜­', 'EMOJI_34': 'ğŸ†–', 'EMOJI_35': 'ğŸ’©', 'EMOJI_36': 'âœ”', 'EMOJI_37': 'ğŸŒ´', 'EMOJI_38': 'âœ…', 'EMOJI_39': 'âŒ', 'EMOJI_40': 'ğŸ˜', 'EMOJI_41': 'ğŸ˜ˆ', 'EMOJI_42': 'ğŸ˜¤', 'EMOJI_43': 'ğŸ‘Œ', 'EMOJI_44': 'ğŸ’ª', 'EMOJI_45': 'ğŸ’”', 'EMOJI_46': 'ğŸ˜ª', 'EMOJI_47': 'ğŸ˜•', 'EMOJI_48': 'ğŸ˜£', 'EMOJI_49': 'ğŸ˜¬', 'EMOJI_50': 'ğŸ˜‹', 'EMOJI_51': 'ğŸ™Œ', 'EMOJI_52': 'ğŸ˜', 'EMOJI_53': 'ğŸŒŸ', 'EMOJI_54': 'ğŸ“±', 'EMOJI_55': 'ğŸ»', 'EMOJI_56': 'ğŸ’–', 'EMOJI_57': 'ğŸ’', 'EMOJI_58': 'ğŸ˜œ', 'EMOJI_59': 'ğŸ˜·', 'EMOJI_60': 'ğŸ˜±', 'EMOJI_61': 'ğŸ˜–', 'EMOJI_62': 'â­', 'EMOJI_63': 'ğŸµ', 'EMOJI_64': 'â—', 'EMOJI_65': 'ğŸ´', 'EMOJI_66': 'ğŸ˜†', 'EMOJI_67': 'ğŸ˜©', 'EMOJI_68': 'ğŸ˜‘', 'EMOJI_69': 'ğŸ˜', 'EMOJI_70': 'â¤´', 'EMOJI_71': 'â˜€', 'EMOJI_72': 'ğŸ‘Š', 'EMOJI_73': 'ğŸ’¯', 'EMOJI_74': 'ğŸ˜ ', 'EMOJI_75': 'â˜•', 'EMOJI_76': 'ğŸ“²', 'EMOJI_77': 'ğŸ‘º', 'EMOJI_78': 'ğŸ™ˆ', 'EMOJI_79': 'ğŸ’˜', 'EMOJI_80': 'ğŸ‘‰', 'EMOJI_81': 'ğŸšª', 'EMOJI_82': 'ğŸ™…', 'EMOJI_83': 'ğŸ˜µ', 'EMOJI_84': 'ğŸ”µ', 'EMOJI_85': 'ğŸ’™', 'EMOJI_86': 'ğŸ‘€', 'EMOJI_87': 'ğŸ˜', 'EMOJI_88': 'ğŸ†˜', 'EMOJI_89': 'ğŸ˜˜', 'EMOJI_90': 'âœ¨', 'EMOJI_91': 'ğŸ˜“', 'EMOJI_92': 'âŒš', 'EMOJI_93': 'ğŸ˜³', 'EMOJI_94': 'ğŸ³', 'EMOJI_95': 'â¤µ', 'EMOJI_96': 'ğŸ‘ ', 'EMOJI_97': 'ğŸŒ', 'EMOJI_98': 'ğŸ˜²', 'EMOJI_99': 'ğŸ˜¦', 'EMOJI_100': 'â¡'}\n"
     ]
    }
   ],
   "source": [
    "print(reverse_lookup_emoji_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and encoded version to pickle.\n",
    "enc_emoji_dict = {}\n",
    "for k,v in emoji_dict.items():\n",
    "    k_enc = k.encode('unicode-escape')\n",
    "    enc_emoji_dict[k_enc]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{b'\\\\u2764': 'EMOJI_1', b'\\\\u263a': 'EMOJI_2', b'\\\\U0001f44d': 'EMOJI_3', b'\\\\U0001f621': 'EMOJI_4', b'\\\\U0001f622': 'EMOJI_5', b'\\\\U0001f49c': 'EMOJI_6', b'\\\\u2708': 'EMOJI_7', b'\\\\U0001f377': 'EMOJI_8', b'\\\\U0001f4ba': 'EMOJI_9', b'\\\\U0001f495': 'EMOJI_10', b'\\\\U0001f601': 'EMOJI_11', b'\\\\U0001f44f': 'EMOJI_12', b'\\\\U0001f602': 'EMOJI_13', b'\\\\U0001f497': 'EMOJI_14', b'\\\\U0001f378': 'EMOJI_15', b'\\\\U0001f612': 'EMOJI_16', b'\\\\U0001f44e': 'EMOJI_17', b'\\\\U0001f60a': 'EMOJI_18', b'\\\\U0001f600': 'EMOJI_19', b'\\\\U0001f603': 'EMOJI_20', b'\\\\U0001f604': 'EMOJI_21', b'\\\\U0001f60e': 'EMOJI_22', b'\\\\U0001f478': 'EMOJI_23', b'\\\\U0001f625': 'EMOJI_24', b'\\\\U0001f380': 'EMOJI_25', b'\\\\U0001f389': 'EMOJI_26', b'\\\\U0001f44b': 'EMOJI_27', b'\\\\u270c': 'EMOJI_28', b'\\\\U0001f64f': 'EMOJI_29', b'\\\\U0001f47f': 'EMOJI_30', b'\\\\U0001f609': 'EMOJI_31', b'\\\\U0001f614': 'EMOJI_32', b'\\\\U0001f62d': 'EMOJI_33', b'\\\\U0001f196': 'EMOJI_34', b'\\\\U0001f4a9': 'EMOJI_35', b'\\\\u2714': 'EMOJI_36', b'\\\\U0001f334': 'EMOJI_37', b'\\\\u2705': 'EMOJI_38', b'\\\\u274c': 'EMOJI_39', b'\\\\U0001f61e': 'EMOJI_40', b'\\\\U0001f608': 'EMOJI_41', b'\\\\U0001f624': 'EMOJI_42', b'\\\\U0001f44c': 'EMOJI_43', b'\\\\U0001f4aa': 'EMOJI_44', b'\\\\U0001f494': 'EMOJI_45', b'\\\\U0001f62a': 'EMOJI_46', b'\\\\U0001f615': 'EMOJI_47', b'\\\\U0001f623': 'EMOJI_48', b'\\\\U0001f62c': 'EMOJI_49', b'\\\\U0001f60b': 'EMOJI_50', b'\\\\U0001f64c': 'EMOJI_51', b'\\\\U0001f60f': 'EMOJI_52', b'\\\\U0001f31f': 'EMOJI_53', b'\\\\U0001f4f1': 'EMOJI_54', b'\\\\U0001f37b': 'EMOJI_55', b'\\\\U0001f496': 'EMOJI_56', b'\\\\U0001f49d': 'EMOJI_57', b'\\\\U0001f61c': 'EMOJI_58', b'\\\\U0001f637': 'EMOJI_59', b'\\\\U0001f631': 'EMOJI_60', b'\\\\U0001f616': 'EMOJI_61', b'\\\\u2b50': 'EMOJI_62', b'\\\\U0001f3b5': 'EMOJI_63', b'\\\\u2757': 'EMOJI_64', b'\\\\U0001f434': 'EMOJI_65', b'\\\\U0001f606': 'EMOJI_66', b'\\\\U0001f629': 'EMOJI_67', b'\\\\U0001f611': 'EMOJI_68', b'\\\\U0001f60d': 'EMOJI_69', b'\\\\u2934': 'EMOJI_70', b'\\\\u2600': 'EMOJI_71', b'\\\\U0001f44a': 'EMOJI_72', b'\\\\U0001f4af': 'EMOJI_73', b'\\\\U0001f620': 'EMOJI_74', b'\\\\u2615': 'EMOJI_75', b'\\\\U0001f4f2': 'EMOJI_76', b'\\\\U0001f47a': 'EMOJI_77', b'\\\\U0001f648': 'EMOJI_78', b'\\\\U0001f498': 'EMOJI_79', b'\\\\U0001f449': 'EMOJI_80', b'\\\\U0001f6aa': 'EMOJI_81', b'\\\\U0001f645': 'EMOJI_82', b'\\\\U0001f635': 'EMOJI_83', b'\\\\U0001f535': 'EMOJI_84', b'\\\\U0001f499': 'EMOJI_85', b'\\\\U0001f440': 'EMOJI_86', b'\\\\U0001f610': 'EMOJI_87', b'\\\\U0001f198': 'EMOJI_88', b'\\\\U0001f618': 'EMOJI_89', b'\\\\u2728': 'EMOJI_90', b'\\\\U0001f613': 'EMOJI_91', b'\\\\u231a': 'EMOJI_92', b'\\\\U0001f633': 'EMOJI_93', b'\\\\U0001f433': 'EMOJI_94', b'\\\\u2935': 'EMOJI_95', b'\\\\U0001f460': 'EMOJI_96', b'\\\\U0001f31e': 'EMOJI_97', b'\\\\U0001f632': 'EMOJI_98', b'\\\\U0001f626': 'EMOJI_99', b'\\\\u27a1': 'EMOJI_100'}\n"
     ]
    }
   ],
   "source": [
    "print(enc_emoji_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/pickled/Emoticon_NB4/'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = data_out_dir+'reverse_lookup_emoji_dict.obj'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle  emoji dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename,'wb') as handle:\n",
    "    pickle.dump(reverse_lookup_emoji_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(filename,'rb')\n",
    "object_content = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EMOJI_1': 'â¤', 'EMOJI_2': 'â˜º', 'EMOJI_3': 'ğŸ‘', 'EMOJI_4': 'ğŸ˜¡', 'EMOJI_5': 'ğŸ˜¢', 'EMOJI_6': 'ğŸ’œ', 'EMOJI_7': 'âœˆ', 'EMOJI_8': 'ğŸ·', 'EMOJI_9': 'ğŸ’º', 'EMOJI_10': 'ğŸ’•', 'EMOJI_11': 'ğŸ˜', 'EMOJI_12': 'ğŸ‘', 'EMOJI_13': 'ğŸ˜‚', 'EMOJI_14': 'ğŸ’—', 'EMOJI_15': 'ğŸ¸', 'EMOJI_16': 'ğŸ˜’', 'EMOJI_17': 'ğŸ‘', 'EMOJI_18': 'ğŸ˜Š', 'EMOJI_19': 'ğŸ˜€', 'EMOJI_20': 'ğŸ˜ƒ', 'EMOJI_21': 'ğŸ˜„', 'EMOJI_22': 'ğŸ˜', 'EMOJI_23': 'ğŸ‘¸', 'EMOJI_24': 'ğŸ˜¥', 'EMOJI_25': 'ğŸ€', 'EMOJI_26': 'ğŸ‰', 'EMOJI_27': 'ğŸ‘‹', 'EMOJI_28': 'âœŒ', 'EMOJI_29': 'ğŸ™', 'EMOJI_30': 'ğŸ‘¿', 'EMOJI_31': 'ğŸ˜‰', 'EMOJI_32': 'ğŸ˜”', 'EMOJI_33': 'ğŸ˜­', 'EMOJI_34': 'ğŸ†–', 'EMOJI_35': 'ğŸ’©', 'EMOJI_36': 'âœ”', 'EMOJI_37': 'ğŸŒ´', 'EMOJI_38': 'âœ…', 'EMOJI_39': 'âŒ', 'EMOJI_40': 'ğŸ˜', 'EMOJI_41': 'ğŸ˜ˆ', 'EMOJI_42': 'ğŸ˜¤', 'EMOJI_43': 'ğŸ‘Œ', 'EMOJI_44': 'ğŸ’ª', 'EMOJI_45': 'ğŸ’”', 'EMOJI_46': 'ğŸ˜ª', 'EMOJI_47': 'ğŸ˜•', 'EMOJI_48': 'ğŸ˜£', 'EMOJI_49': 'ğŸ˜¬', 'EMOJI_50': 'ğŸ˜‹', 'EMOJI_51': 'ğŸ™Œ', 'EMOJI_52': 'ğŸ˜', 'EMOJI_53': 'ğŸŒŸ', 'EMOJI_54': 'ğŸ“±', 'EMOJI_55': 'ğŸ»', 'EMOJI_56': 'ğŸ’–', 'EMOJI_57': 'ğŸ’', 'EMOJI_58': 'ğŸ˜œ', 'EMOJI_59': 'ğŸ˜·', 'EMOJI_60': 'ğŸ˜±', 'EMOJI_61': 'ğŸ˜–', 'EMOJI_62': 'â­', 'EMOJI_63': 'ğŸµ', 'EMOJI_64': 'â—', 'EMOJI_65': 'ğŸ´', 'EMOJI_66': 'ğŸ˜†', 'EMOJI_67': 'ğŸ˜©', 'EMOJI_68': 'ğŸ˜‘', 'EMOJI_69': 'ğŸ˜', 'EMOJI_70': 'â¤´', 'EMOJI_71': 'â˜€', 'EMOJI_72': 'ğŸ‘Š', 'EMOJI_73': 'ğŸ’¯', 'EMOJI_74': 'ğŸ˜ ', 'EMOJI_75': 'â˜•', 'EMOJI_76': 'ğŸ“²', 'EMOJI_77': 'ğŸ‘º', 'EMOJI_78': 'ğŸ™ˆ', 'EMOJI_79': 'ğŸ’˜', 'EMOJI_80': 'ğŸ‘‰', 'EMOJI_81': 'ğŸšª', 'EMOJI_82': 'ğŸ™…', 'EMOJI_83': 'ğŸ˜µ', 'EMOJI_84': 'ğŸ”µ', 'EMOJI_85': 'ğŸ’™', 'EMOJI_86': 'ğŸ‘€', 'EMOJI_87': 'ğŸ˜', 'EMOJI_88': 'ğŸ†˜', 'EMOJI_89': 'ğŸ˜˜', 'EMOJI_90': 'âœ¨', 'EMOJI_91': 'ğŸ˜“', 'EMOJI_92': 'âŒš', 'EMOJI_93': 'ğŸ˜³', 'EMOJI_94': 'ğŸ³', 'EMOJI_95': 'â¤µ', 'EMOJI_96': 'ğŸ‘ ', 'EMOJI_97': 'ğŸŒ', 'EMOJI_98': 'ğŸ˜²', 'EMOJI_99': 'ğŸ˜¦', 'EMOJI_100': 'â¡'}\n"
     ]
    }
   ],
   "source": [
    "print(object_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{b'\\\\u2764': 'EMOJI_1', b'\\\\u263a': 'EMOJI_2', b'\\\\U0001f44d': 'EMOJI_3', b'\\\\U0001f621': 'EMOJI_4', b'\\\\U0001f622': 'EMOJI_5', b'\\\\U0001f49c': 'EMOJI_6', b'\\\\u2708': 'EMOJI_7', b'\\\\U0001f377': 'EMOJI_8', b'\\\\U0001f4ba': 'EMOJI_9', b'\\\\U0001f495': 'EMOJI_10', b'\\\\U0001f601': 'EMOJI_11', b'\\\\U0001f44f': 'EMOJI_12', b'\\\\U0001f602': 'EMOJI_13', b'\\\\U0001f497': 'EMOJI_14', b'\\\\U0001f378': 'EMOJI_15', b'\\\\U0001f612': 'EMOJI_16', b'\\\\U0001f44e': 'EMOJI_17', b'\\\\U0001f60a': 'EMOJI_18', b'\\\\U0001f600': 'EMOJI_19', b'\\\\U0001f603': 'EMOJI_20', b'\\\\U0001f604': 'EMOJI_21', b'\\\\U0001f60e': 'EMOJI_22', b'\\\\U0001f478': 'EMOJI_23', b'\\\\U0001f625': 'EMOJI_24', b'\\\\U0001f380': 'EMOJI_25', b'\\\\U0001f389': 'EMOJI_26', b'\\\\U0001f44b': 'EMOJI_27', b'\\\\u270c': 'EMOJI_28', b'\\\\U0001f64f': 'EMOJI_29', b'\\\\U0001f47f': 'EMOJI_30', b'\\\\U0001f609': 'EMOJI_31', b'\\\\U0001f614': 'EMOJI_32', b'\\\\U0001f62d': 'EMOJI_33', b'\\\\U0001f196': 'EMOJI_34', b'\\\\U0001f4a9': 'EMOJI_35', b'\\\\u2714': 'EMOJI_36', b'\\\\U0001f334': 'EMOJI_37', b'\\\\u2705': 'EMOJI_38', b'\\\\u274c': 'EMOJI_39', b'\\\\U0001f61e': 'EMOJI_40', b'\\\\U0001f608': 'EMOJI_41', b'\\\\U0001f624': 'EMOJI_42', b'\\\\U0001f44c': 'EMOJI_43', b'\\\\U0001f4aa': 'EMOJI_44', b'\\\\U0001f494': 'EMOJI_45', b'\\\\U0001f62a': 'EMOJI_46', b'\\\\U0001f615': 'EMOJI_47', b'\\\\U0001f623': 'EMOJI_48', b'\\\\U0001f62c': 'EMOJI_49', b'\\\\U0001f60b': 'EMOJI_50', b'\\\\U0001f64c': 'EMOJI_51', b'\\\\U0001f60f': 'EMOJI_52', b'\\\\U0001f31f': 'EMOJI_53', b'\\\\U0001f4f1': 'EMOJI_54', b'\\\\U0001f37b': 'EMOJI_55', b'\\\\U0001f496': 'EMOJI_56', b'\\\\U0001f49d': 'EMOJI_57', b'\\\\U0001f61c': 'EMOJI_58', b'\\\\U0001f637': 'EMOJI_59', b'\\\\U0001f631': 'EMOJI_60', b'\\\\U0001f616': 'EMOJI_61', b'\\\\u2b50': 'EMOJI_62', b'\\\\U0001f3b5': 'EMOJI_63', b'\\\\u2757': 'EMOJI_64', b'\\\\U0001f434': 'EMOJI_65', b'\\\\U0001f606': 'EMOJI_66', b'\\\\U0001f629': 'EMOJI_67', b'\\\\U0001f611': 'EMOJI_68', b'\\\\U0001f60d': 'EMOJI_69', b'\\\\u2934': 'EMOJI_70', b'\\\\u2600': 'EMOJI_71', b'\\\\U0001f44a': 'EMOJI_72', b'\\\\U0001f4af': 'EMOJI_73', b'\\\\U0001f620': 'EMOJI_74', b'\\\\u2615': 'EMOJI_75', b'\\\\U0001f4f2': 'EMOJI_76', b'\\\\U0001f47a': 'EMOJI_77', b'\\\\U0001f648': 'EMOJI_78', b'\\\\U0001f498': 'EMOJI_79', b'\\\\U0001f449': 'EMOJI_80', b'\\\\U0001f6aa': 'EMOJI_81', b'\\\\U0001f645': 'EMOJI_82', b'\\\\U0001f635': 'EMOJI_83', b'\\\\U0001f535': 'EMOJI_84', b'\\\\U0001f499': 'EMOJI_85', b'\\\\U0001f440': 'EMOJI_86', b'\\\\U0001f610': 'EMOJI_87', b'\\\\U0001f198': 'EMOJI_88', b'\\\\U0001f618': 'EMOJI_89', b'\\\\u2728': 'EMOJI_90', b'\\\\U0001f613': 'EMOJI_91', b'\\\\u231a': 'EMOJI_92', b'\\\\U0001f633': 'EMOJI_93', b'\\\\U0001f433': 'EMOJI_94', b'\\\\u2935': 'EMOJI_95', b'\\\\U0001f460': 'EMOJI_96', b'\\\\U0001f31e': 'EMOJI_97', b'\\\\U0001f632': 'EMOJI_98', b'\\\\U0001f626': 'EMOJI_99', b'\\\\u27a1': 'EMOJI_100'}\n"
     ]
    }
   ],
   "source": [
    "print(object_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'â¤': 'EMOJI_1', 'â˜º': 'EMOJI_2', 'ğŸ‘': 'EMOJI_3', 'ğŸ˜¡': 'EMOJI_4', 'ğŸ˜¢': 'EMOJI_5', 'ğŸ’œ': 'EMOJI_6', 'âœˆ': 'EMOJI_7', 'ğŸ·': 'EMOJI_8', 'ğŸ’º': 'EMOJI_9', 'ğŸ’•': 'EMOJI_10', 'ğŸ˜': 'EMOJI_11', 'ğŸ‘': 'EMOJI_12', 'ğŸ˜‚': 'EMOJI_13', 'ğŸ’—': 'EMOJI_14', 'ğŸ¸': 'EMOJI_15', 'ğŸ˜’': 'EMOJI_16', 'ğŸ‘': 'EMOJI_17', 'ğŸ˜Š': 'EMOJI_18', 'ğŸ˜€': 'EMOJI_19', 'ğŸ˜ƒ': 'EMOJI_20', 'ğŸ˜„': 'EMOJI_21', 'ğŸ˜': 'EMOJI_22', 'ğŸ‘¸': 'EMOJI_23', 'ğŸ˜¥': 'EMOJI_24', 'ğŸ€': 'EMOJI_25', 'ğŸ‰': 'EMOJI_26', 'ğŸ‘‹': 'EMOJI_27', 'âœŒ': 'EMOJI_28', 'ğŸ™': 'EMOJI_29', 'ğŸ‘¿': 'EMOJI_30', 'ğŸ˜‰': 'EMOJI_31', 'ğŸ˜”': 'EMOJI_32', 'ğŸ˜­': 'EMOJI_33', 'ğŸ†–': 'EMOJI_34', 'ğŸ’©': 'EMOJI_35', 'âœ”': 'EMOJI_36', 'ğŸŒ´': 'EMOJI_37', 'âœ…': 'EMOJI_38', 'âŒ': 'EMOJI_39', 'ğŸ˜': 'EMOJI_40', 'ğŸ˜ˆ': 'EMOJI_41', 'ğŸ˜¤': 'EMOJI_42', 'ğŸ‘Œ': 'EMOJI_43', 'ğŸ’ª': 'EMOJI_44', 'ğŸ’”': 'EMOJI_45', 'ğŸ˜ª': 'EMOJI_46', 'ğŸ˜•': 'EMOJI_47', 'ğŸ˜£': 'EMOJI_48', 'ğŸ˜¬': 'EMOJI_49', 'ğŸ˜‹': 'EMOJI_50', 'ğŸ™Œ': 'EMOJI_51', 'ğŸ˜': 'EMOJI_52', 'ğŸŒŸ': 'EMOJI_53', 'ğŸ“±': 'EMOJI_54', 'ğŸ»': 'EMOJI_55', 'ğŸ’–': 'EMOJI_56', 'ğŸ’': 'EMOJI_57', 'ğŸ˜œ': 'EMOJI_58', 'ğŸ˜·': 'EMOJI_59', 'ğŸ˜±': 'EMOJI_60', 'ğŸ˜–': 'EMOJI_61', 'â­': 'EMOJI_62', 'ğŸµ': 'EMOJI_63', 'â—': 'EMOJI_64', 'ğŸ´': 'EMOJI_65', 'ğŸ˜†': 'EMOJI_66', 'ğŸ˜©': 'EMOJI_67', 'ğŸ˜‘': 'EMOJI_68', 'ğŸ˜': 'EMOJI_69', 'â¤´': 'EMOJI_70', 'â˜€': 'EMOJI_71', 'ğŸ‘Š': 'EMOJI_72', 'ğŸ’¯': 'EMOJI_73', 'ğŸ˜ ': 'EMOJI_74', 'â˜•': 'EMOJI_75', 'ğŸ“²': 'EMOJI_76', 'ğŸ‘º': 'EMOJI_77', 'ğŸ™ˆ': 'EMOJI_78', 'ğŸ’˜': 'EMOJI_79', 'ğŸ‘‰': 'EMOJI_80', 'ğŸšª': 'EMOJI_81', 'ğŸ™…': 'EMOJI_82', 'ğŸ˜µ': 'EMOJI_83', 'ğŸ”µ': 'EMOJI_84', 'ğŸ’™': 'EMOJI_85', 'ğŸ‘€': 'EMOJI_86', 'ğŸ˜': 'EMOJI_87', 'ğŸ†˜': 'EMOJI_88', 'ğŸ˜˜': 'EMOJI_89', 'âœ¨': 'EMOJI_90', 'ğŸ˜“': 'EMOJI_91', 'âŒš': 'EMOJI_92', 'ğŸ˜³': 'EMOJI_93', 'ğŸ³': 'EMOJI_94', 'â¤µ': 'EMOJI_95', 'ğŸ‘ ': 'EMOJI_96', 'ğŸŒ': 'EMOJI_97', 'ğŸ˜²': 'EMOJI_98', 'ğŸ˜¦': 'EMOJI_99', 'â¡': 'EMOJI_100'}\n"
     ]
    }
   ],
   "source": [
    "print(object_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here down in the notebook is just work and things I tried, trying to understand how to deal with emojis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'â¤'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_em = uni_single_emoji[0]\n",
    "char_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\\\u2764'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = char_em.encode('unicode-escape')\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'â¤'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = enc.decode('unicode-escape')\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can I make a dict with the char as key?\n",
    "a = {char_em:'EMOJI_1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EMOJI_1'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[char_em]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I really like to travel â¤'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \" I really like to travel\"\n",
    "b = a + ' ' + char_em\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b' I really like to travel \\\\u2764'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_enc = b.encode('unicode-escape')\n",
    "b_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I really like to travel â¤'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_enc.decode('unicode-escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub('b\\\\u2764', 'heart', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I really like to travel heart'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['really', 'like', 'to', 'travel', 'heart']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('(?u)\\\\b\\\\w\\\\w+\\\\b', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i â¤ï¸ flying AT_USER â˜ºï¸ğŸ‘'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = corpus.values[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'â¤ â˜º ğŸ‘'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_em = extract_emojis(text)\n",
    "extracted_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â¤', 'â˜º', 'ğŸ‘']\n"
     ]
    }
   ],
   "source": [
    "print(extracted_em.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¤\n",
      "â˜º\n",
      "ğŸ‘\n"
     ]
    }
   ],
   "source": [
    "text = corpus.values[0]\n",
    "extracted_em = extract_emojis(text)\n",
    "for i in (extracted_em.split()):\n",
    "    if i in uni_single_emoji:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can identify if the emoji in new text is present in our unique emojis list.\n",
    "# however first we need to extract the emojis then split them into individual chars\n",
    "# since when user writes they bunch them up together.\n",
    "\n",
    "# I want to individually identify them as individual features, not as a bunched up \n",
    "# features. * Although, in the future this may be an add on *."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that I know the process, I want tokenize the emojis in the cleanup tweet phase and append \n",
    "# it to the cleaned tweet. Let's exlplore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>emojis</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I â¤ï¸ flying @VirginAmerica. â˜ºï¸ğŸ‘</td>\n",
       "      <td>i â¤ï¸ flying AT_USER â˜ºï¸ğŸ‘</td>\n",
       "      <td>â¤ â˜º ğŸ‘</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@VirginAmerica you guys messed up my seating.....</td>\n",
       "      <td>AT_USER you guys messed up my seating.. i rese...</td>\n",
       "      <td>ğŸ˜¡</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@VirginAmerica hi! I just bked a cool birthday...</td>\n",
       "      <td>AT_USER hi! i just bked a cool birthday trip w...</td>\n",
       "      <td>ğŸ˜¢</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>@VirginAmerica Moodlighting is the only way to...</td>\n",
       "      <td>AT_USER moodlighting is the only way to fly! b...</td>\n",
       "      <td>ğŸ’œ âœˆ</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>@VirginAmerica plz help me win my bid upgrade ...</td>\n",
       "      <td>AT_USER plz help me win my bid upgrade for my ...</td>\n",
       "      <td>ğŸ· ğŸ‘ ğŸ’º âœˆ</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>@VirginAmerica - amazing customer  service, ag...</td>\n",
       "      <td>AT_USER - amazing customer service, again! ğŸ’•ğŸ’• ...</td>\n",
       "      <td>ğŸ’• ğŸ’•</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>@VirginAmerica trying to book a flight &amp;amp; y...</td>\n",
       "      <td>AT_USER trying to book a flight &amp;amp; your sit...</td>\n",
       "      <td>ğŸ˜</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>@VirginAmerica my goodness your people @love f...</td>\n",
       "      <td>AT_USER my goodness your people AT_USER field ...</td>\n",
       "      <td>â¤</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>@VirginAmerica Very nicely done. ğŸ‘</td>\n",
       "      <td>AT_USER very nicely done. ğŸ‘</td>\n",
       "      <td>ğŸ‘</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>@VirginAmerica hahaha ğŸ˜‚@VirginAmerica YOU GUYS...</td>\n",
       "      <td>AT_USER hahaha ğŸ˜‚AT_USER you guys are amazing. ...</td>\n",
       "      <td>ğŸ˜‚ ğŸ’—</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "10                     I â¤ï¸ flying @VirginAmerica. â˜ºï¸ğŸ‘   \n",
       "15   @VirginAmerica you guys messed up my seating.....   \n",
       "19   @VirginAmerica hi! I just bked a cool birthday...   \n",
       "24   @VirginAmerica Moodlighting is the only way to...   \n",
       "27   @VirginAmerica plz help me win my bid upgrade ...   \n",
       "73   @VirginAmerica - amazing customer  service, ag...   \n",
       "138  @VirginAmerica trying to book a flight &amp; y...   \n",
       "146  @VirginAmerica my goodness your people @love f...   \n",
       "164                 @VirginAmerica Very nicely done. ğŸ‘   \n",
       "165  @VirginAmerica hahaha ğŸ˜‚@VirginAmerica YOU GUYS...   \n",
       "\n",
       "                                            clean_text   emojis  \\\n",
       "10                             i â¤ï¸ flying AT_USER â˜ºï¸ğŸ‘    â¤ â˜º ğŸ‘   \n",
       "15   AT_USER you guys messed up my seating.. i rese...        ğŸ˜¡   \n",
       "19   AT_USER hi! i just bked a cool birthday trip w...        ğŸ˜¢   \n",
       "24   AT_USER moodlighting is the only way to fly! b...      ğŸ’œ âœˆ   \n",
       "27   AT_USER plz help me win my bid upgrade for my ...  ğŸ· ğŸ‘ ğŸ’º âœˆ   \n",
       "73   AT_USER - amazing customer service, again! ğŸ’•ğŸ’• ...      ğŸ’• ğŸ’•   \n",
       "138  AT_USER trying to book a flight &amp; your sit...        ğŸ˜   \n",
       "146  AT_USER my goodness your people AT_USER field ...        â¤   \n",
       "164                        AT_USER very nicely done. ğŸ‘        ğŸ‘   \n",
       "165  AT_USER hahaha ğŸ˜‚AT_USER you guys are amazing. ...      ğŸ˜‚ ğŸ’—   \n",
       "\n",
       "    airline_sentiment  \n",
       "10           positive  \n",
       "15           negative  \n",
       "19           negative  \n",
       "24           positive  \n",
       "27            neutral  \n",
       "73           positive  \n",
       "138          negative  \n",
       "146          positive  \n",
       "164          positive  \n",
       "165          positive  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# My dataset dataframe includes a sperate col with a string of emojis found in the text.\n",
    "# clean_text has text cleaned from html tags, @user replaced... refer to EDA_NB1 and EDA_NB2 \n",
    "# notebooks.\n",
    "df_emoji[['text', 'clean_text', 'emojis', 'airline_sentiment']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i â¤ï¸ flying AT_USER â˜ºï¸ğŸ‘'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df_emoji.clean_text.values[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'â¤ â˜º ğŸ‘'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_em = extract_emojis(text)\n",
    "extracted_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'â¤ï¸', 'flying', 'AT_USER', 'â˜ºï¸ğŸ‘']"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individualize_emojis(text):\n",
    "    em_txt = text        \n",
    "    for i in text:\n",
    "        if (i in emoji.UNICODE_EMOJI):\n",
    "            print(i)\n",
    "            em_txt = em_txt.replace(i,' '+i+' ')\n",
    "        \n",
    "    return(em_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_emoji.clean_text.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¤\n",
      "â˜º\n",
      "ğŸ‘\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i  â¤ ï¸ flying AT_USER  â˜º ï¸ ğŸ‘ '"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=individualize_emojis(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'â¤', 'ï¸', 'flying', 'AT_USER', 'â˜º', 'ï¸', 'ğŸ‘']"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the emoji features are individual features. Let's create a new column in the dataset dataframe with \n",
    "# text that represents emojis as a separate character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¤\n",
      "â˜º\n",
      "ğŸ‘\n",
      "ğŸ˜¡\n",
      "ğŸ˜¢\n",
      "ğŸ’œ\n",
      "âœˆ\n",
      "ğŸ·\n",
      "ğŸ‘\n",
      "ğŸ’º\n",
      "âœˆ\n",
      "ğŸ’•\n",
      "ğŸ’•\n",
      "ğŸ˜\n",
      "â¤\n",
      "ğŸ‘\n",
      "ğŸ˜‚\n",
      "ğŸ’—\n",
      "ğŸ¸\n",
      "ğŸ˜’\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "âœˆ\n",
      "âœˆ\n",
      "ğŸ’—\n",
      "ğŸ˜Š\n",
      "ğŸ˜€\n",
      "ğŸ˜ƒ\n",
      "ğŸ˜„\n",
      "ğŸ˜\n",
      "ğŸ‘¸\n",
      "ğŸ’—\n",
      "ğŸ˜¥\n",
      "ğŸ€\n",
      "ğŸ€\n",
      "ğŸ€\n",
      "âœˆ\n",
      "ğŸ‰\n",
      "ğŸ’—\n",
      "ğŸ€\n",
      "ğŸ’—\n",
      "ğŸ˜ƒ\n",
      "ğŸ‘\n",
      "ğŸ˜¡\n",
      "ğŸ‘‹\n",
      "ğŸ˜¢\n",
      "âœŒ\n",
      "ğŸ™\n",
      "â¤\n",
      "ğŸ’œ\n",
      "ğŸ‘¿\n",
      "ğŸ˜‰\n",
      "ğŸ˜‰\n",
      "ğŸ˜”\n",
      "ğŸ˜­\n",
      "ğŸ˜Š\n",
      "âœˆ\n",
      "ğŸ‘\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "âœˆ\n",
      "ğŸ‘\n",
      "ğŸ†–\n",
      "ğŸ˜¢\n",
      "ğŸ’©\n",
      "âœ”\n",
      "ğŸŒ´\n",
      "ğŸŒ´\n",
      "âœ…\n",
      "âŒ\n",
      "ğŸ˜¡\n",
      "ğŸ˜Š\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ™\n",
      "ğŸ˜„\n",
      "ğŸ˜„\n",
      "ğŸ˜„\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜’\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ˜\n",
      "ğŸ˜¡\n",
      "ğŸ˜Š\n",
      "ğŸ‰\n",
      "ğŸ‰\n",
      "ğŸ‰\n",
      "ğŸ˜”\n",
      "ğŸ˜Š\n",
      "ğŸ˜­\n",
      "ğŸ‘\n",
      "ğŸ˜‰\n",
      "ğŸ˜¢\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ˜ˆ\n",
      "ğŸ˜¡\n",
      "ğŸ˜Š\n",
      "ğŸ‘\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜¤\n",
      "ğŸ˜¤\n",
      "ğŸ˜¤\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ˜Š\n",
      "â¤\n",
      "ğŸ‘\n",
      "ğŸ‘Œ\n",
      "ğŸ˜‚\n",
      "ğŸ˜€\n",
      "ğŸ‘Œ\n",
      "ğŸ’ª\n",
      "ğŸ˜‰\n",
      "ğŸ’”\n",
      "ğŸ˜ª\n",
      "ğŸ˜•\n",
      "ğŸ˜£\n",
      "ğŸ˜¬\n",
      "ğŸ‘\n",
      "ğŸ˜„\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ˜‹\n",
      "ğŸ™Œ\n",
      "ğŸ˜\n",
      "ğŸŒŸ\n",
      "ğŸŒŸ\n",
      "âœˆ\n",
      "âœˆ\n",
      "ğŸ“±\n",
      "ğŸ˜\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ»\n",
      "ğŸ»\n",
      "ğŸ‘\n",
      "ğŸ˜\n",
      "ğŸ˜¡\n",
      "ğŸ’–\n",
      "ğŸ˜”\n",
      "ğŸ˜”\n",
      "ğŸ˜”\n",
      "ğŸ˜¢\n",
      "ğŸ’\n",
      "ğŸ˜\n",
      "ğŸ˜œ\n",
      "ğŸ˜‚\n",
      "ğŸ˜·\n",
      "ğŸ˜±\n",
      "ğŸ˜\n",
      "ğŸ˜–\n",
      "â­\n",
      "âœˆ\n",
      "ğŸ‘\n",
      "ğŸ˜ƒ\n",
      "ğŸ’•\n",
      "ğŸµ\n",
      "âœˆ\n",
      "â—\n",
      "â¤\n",
      "ğŸ˜¤\n",
      "ğŸ´\n",
      "ğŸ˜¢\n",
      "âœˆ\n",
      "ğŸ˜\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ’”\n",
      "ğŸ’”\n",
      "ğŸ’”\n",
      "ğŸ’”\n",
      "ğŸ’”\n",
      "ğŸ’”\n",
      "ğŸ’”\n",
      "ğŸ˜\n",
      "ğŸ˜‰\n",
      "â¤\n",
      "ğŸ˜¥\n",
      "ğŸ˜†\n",
      "ğŸ˜Š\n",
      "ğŸŒ´\n",
      "â¤\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "âœˆ\n",
      "âœˆ\n",
      "âœˆ\n",
      "ğŸ˜ƒ\n",
      "ğŸ‘\n",
      "â¤\n",
      "â¤\n",
      "â¤\n",
      "ğŸ˜©\n",
      "ğŸ˜‘\n",
      "ğŸ’•\n",
      "â¤\n",
      "ğŸ˜ƒ\n",
      "ğŸ’•\n",
      "ğŸ˜\n",
      "â¤´\n",
      "â¤´\n",
      "ğŸ˜’\n",
      "ğŸ‘\n",
      "ğŸ˜œ\n",
      "â˜€\n",
      "ğŸ˜­\n",
      "â¤\n",
      "ğŸ‘Š\n",
      "ğŸ˜„\n",
      "ğŸ˜ƒ\n",
      "ğŸ˜€\n",
      "âœˆ\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ‰\n",
      "ğŸ‰\n",
      "ğŸ‰\n",
      "â¤\n",
      "ğŸ’¯\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ‘\n",
      "âœˆ\n",
      "ğŸ’©\n",
      "ğŸ’©\n",
      "ğŸ’©\n",
      "ğŸ’©\n",
      "ğŸ˜ \n",
      "ğŸ˜ \n",
      "ğŸ˜”\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜Š\n",
      "â˜•\n",
      "ğŸ“²\n",
      "âœˆ\n",
      "ğŸ˜ \n",
      "ğŸ‘º\n",
      "ğŸ™ˆ\n",
      "ğŸ’˜\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ˜\n",
      "ğŸ˜‚\n",
      "ğŸ‘‰\n",
      "ğŸšª\n",
      "ğŸ™…\n",
      "ğŸ˜‚\n",
      "ğŸ˜‚\n",
      "ğŸ˜‚\n",
      "ğŸ˜­\n",
      "ğŸ˜\n",
      "ğŸ˜†\n",
      "ğŸ˜µ\n",
      "ğŸ˜’\n",
      "âœˆ\n",
      "ğŸ”µ\n",
      "ğŸ”µ\n",
      "ğŸ”µ\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜‚\n",
      "ğŸ˜‚\n",
      "ğŸ˜‘\n",
      "ğŸ’™\n",
      "ğŸ‘€\n",
      "ğŸ‘€\n",
      "ğŸ˜’\n",
      "ğŸ˜‚\n",
      "ğŸ˜‚\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ‘€\n",
      "ğŸ˜\n",
      "ğŸ˜‘\n",
      "ğŸ˜Š\n",
      "ğŸ˜Š\n",
      "ğŸ’™\n",
      "ğŸ’™\n",
      "ğŸ’™\n",
      "ğŸ’™\n",
      "ğŸ˜‚\n",
      "ğŸ‘Œ\n",
      "ğŸ‘Œ\n",
      "ğŸ‘Œ\n",
      "ğŸ˜’\n",
      "â¤\n",
      "ğŸ˜­\n",
      "ğŸ˜’\n",
      "ğŸ˜\n",
      "ğŸ˜\n",
      "ğŸ˜\n",
      "ğŸ˜ƒ\n",
      "ğŸ˜‰\n",
      "â˜º\n",
      "âœˆ\n",
      "ğŸ™Œ\n",
      "âœˆ\n",
      "ğŸ˜•\n",
      "ğŸŒ´\n",
      "â˜º\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ˜Š\n",
      "ğŸ˜’\n",
      "ğŸ˜Š\n",
      "ğŸ˜Š\n",
      "ğŸ‘\n",
      "ğŸ˜\n",
      "ğŸ‰\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ˜˜\n",
      "ğŸ˜\n",
      "ğŸ˜¢\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ˜Š\n",
      "ğŸ˜­\n",
      "ğŸ’”\n",
      "ğŸ‘\n",
      "ğŸ˜·\n",
      "ğŸ˜‰\n",
      "â¤\n",
      "ğŸ˜\n",
      "ğŸ˜„\n",
      "ğŸ’™\n",
      "ğŸ’™\n",
      "ğŸ’™\n",
      "ğŸ˜ƒ\n",
      "ğŸ˜Š\n",
      "âœˆ\n",
      "ğŸ˜”\n",
      "â¤\n",
      "âœ¨\n",
      "ğŸ’™\n",
      "â˜•\n",
      "âœˆ\n",
      "ğŸ‘\n",
      "ğŸ˜¢\n",
      "ğŸ˜¢\n",
      "ğŸ·\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ˜Š\n",
      "ğŸ˜Š\n",
      "ğŸ˜Š\n",
      "âœˆ\n",
      "âœˆ\n",
      "âœˆ\n",
      "ğŸ˜¢\n",
      "ğŸ‘Œ\n",
      "â˜º\n",
      "ğŸ˜©\n",
      "ğŸŒ´\n",
      "ğŸ’™\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ’•\n",
      "âœˆ\n",
      "ğŸ’º\n",
      "âœˆ\n",
      "ğŸ˜”\n",
      "ğŸ˜“\n",
      "ğŸ˜¤\n",
      "â˜º\n",
      "ğŸ˜€\n",
      "ğŸ˜‰\n",
      "ğŸ˜‚\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "â¤\n",
      "ğŸ˜\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜Š\n",
      "ğŸ‘€\n",
      "ğŸ™\n",
      "ğŸ‘\n",
      "ğŸ˜‰\n",
      "ğŸ‘\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ˜’\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜Š\n",
      "âœˆ\n",
      "âŒš\n",
      "ğŸ˜‚\n",
      "ğŸ˜‚\n",
      "ğŸ˜‚\n",
      "ğŸ‘Š\n",
      "ğŸ˜³\n",
      "ğŸ˜¡\n",
      "âœŒ\n",
      "âœŒ\n",
      "ğŸ‘\n",
      "ğŸ˜©\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜ \n",
      "ğŸ˜‘\n",
      "ğŸ˜©\n",
      "ğŸ³\n",
      "ğŸ˜¡\n",
      "ğŸ‘Œ\n",
      "ğŸ˜¡\n",
      "ğŸ˜‚\n",
      "ğŸ’º\n",
      "âœˆ\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "âœŒ\n",
      "âœŒ\n",
      "âœŒ\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ˜Š\n",
      "â¤µ\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "â¤µ\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "âœŒ\n",
      "âœŒ\n",
      "âœŒ\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ‘\n",
      "âœˆ\n",
      "ğŸ‘\n",
      "ğŸ˜¬\n",
      "ğŸ‘\n",
      "ğŸ˜¡\n",
      "ğŸ˜Š\n",
      "ğŸ‘\n",
      "ğŸ˜¤\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "âœŒ\n",
      "âœŒ\n",
      "âœŒ\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ‘¿\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "âœŒ\n",
      "âœŒ\n",
      "âœŒ\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ‘ \n",
      "ğŸ‘ \n",
      "ğŸ‘ \n",
      "ğŸ˜‘\n",
      "ğŸ˜‰\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ˜¢\n",
      "ğŸ˜¢\n",
      "ğŸ˜¢\n",
      "ğŸ˜¢\n",
      "ğŸ˜¢\n",
      "ğŸ˜¢\n",
      "ğŸ˜¢\n",
      "ğŸ˜¢\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ˜¢\n",
      "ğŸ˜¢\n",
      "ğŸ˜¢\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "âœŒ\n",
      "ğŸ˜‰\n",
      "âœˆ\n",
      "ğŸŒ\n",
      "ğŸ˜•\n",
      "ğŸ˜ƒ\n",
      "ğŸ˜„\n",
      "ğŸ˜\n",
      "ğŸ˜„\n",
      "ğŸ˜\n",
      "ğŸ˜’\n",
      "ğŸ˜„\n",
      "ğŸ™\n",
      "ğŸ˜­\n",
      "ğŸ˜€\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜\n",
      "ğŸ‘\n",
      "ğŸ˜¡\n",
      "âœˆ\n",
      "ğŸ‘Œ\n",
      "ğŸ‘Œ\n",
      "ğŸ‘Œ\n",
      "ğŸ‘€\n",
      "ğŸ˜\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "âœˆ\n",
      "ğŸ˜¥\n",
      "ğŸ˜¥\n",
      "âœˆ\n",
      "ğŸ˜‹\n",
      "ğŸ˜¢\n",
      "âœˆ\n",
      "ğŸ˜­\n",
      "ğŸ™\n",
      "ğŸ˜„\n",
      "ğŸ’\n",
      "ğŸ’\n",
      "ğŸ’\n",
      "ğŸ˜¤\n",
      "ğŸ‘\n",
      "ğŸ˜¡\n",
      "ğŸ˜¤\n",
      "ğŸ˜–\n",
      "ğŸ˜²\n",
      "ğŸ˜©\n",
      "ğŸ˜¢\n",
      "ğŸ˜•\n",
      "ğŸ˜¦\n",
      "ğŸ˜³\n",
      "ğŸ˜’\n",
      "ğŸ‘\n",
      "ğŸ˜¡\n",
      "â¤\n",
      "â¤\n",
      "ğŸ˜’\n",
      "ğŸ˜ƒ\n",
      "ğŸ˜“\n",
      "ğŸ˜­\n",
      "âœˆ\n",
      "â¡\n",
      "ğŸ˜•\n",
      "ğŸ˜ƒ\n",
      "âœˆ\n",
      "âœˆ\n",
      "âœˆ\n",
      "âœˆ\n",
      "âœˆ\n",
      "âœˆ\n",
      "âœˆ\n",
      "âœˆ\n",
      "âœŒ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_emoji['clean_emoji_text'] = df_emoji['clean_text'].apply(individualize_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_emoji_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i â¤ï¸ flying AT_USER â˜ºï¸ğŸ‘</td>\n",
       "      <td>i  â¤ ï¸ flying AT_USER  â˜º ï¸ ğŸ‘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AT_USER you guys messed up my seating.. i rese...</td>\n",
       "      <td>AT_USER you guys messed up my seating.. i rese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AT_USER hi! i just bked a cool birthday trip w...</td>\n",
       "      <td>AT_USER hi! i just bked a cool birthday trip w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AT_USER moodlighting is the only way to fly! b...</td>\n",
       "      <td>AT_USER moodlighting is the only way to fly! b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AT_USER plz help me win my bid upgrade for my ...</td>\n",
       "      <td>AT_USER plz help me win my bid upgrade for my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AT_USER - amazing customer service, again! ğŸ’•ğŸ’• ...</td>\n",
       "      <td>AT_USER - amazing customer service, again!   ğŸ’•...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AT_USER trying to book a flight &amp;amp; your sit...</td>\n",
       "      <td>AT_USER trying to book a flight &amp;amp; your sit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AT_USER my goodness your people AT_USER field ...</td>\n",
       "      <td>AT_USER my goodness your people AT_USER field ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AT_USER very nicely done. ğŸ‘</td>\n",
       "      <td>AT_USER very nicely done.  ğŸ‘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AT_USER hahaha ğŸ˜‚AT_USER you guys are amazing. ...</td>\n",
       "      <td>AT_USER hahaha  ğŸ˜‚ AT_USER you guys are amazing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  \\\n",
       "0                            i â¤ï¸ flying AT_USER â˜ºï¸ğŸ‘   \n",
       "1  AT_USER you guys messed up my seating.. i rese...   \n",
       "2  AT_USER hi! i just bked a cool birthday trip w...   \n",
       "3  AT_USER moodlighting is the only way to fly! b...   \n",
       "4  AT_USER plz help me win my bid upgrade for my ...   \n",
       "5  AT_USER - amazing customer service, again! ğŸ’•ğŸ’• ...   \n",
       "6  AT_USER trying to book a flight &amp; your sit...   \n",
       "7  AT_USER my goodness your people AT_USER field ...   \n",
       "8                        AT_USER very nicely done. ğŸ‘   \n",
       "9  AT_USER hahaha ğŸ˜‚AT_USER you guys are amazing. ...   \n",
       "\n",
       "                                    clean_emoji_text  \n",
       "0                      i  â¤ ï¸ flying AT_USER  â˜º ï¸ ğŸ‘   \n",
       "1  AT_USER you guys messed up my seating.. i rese...  \n",
       "2  AT_USER hi! i just bked a cool birthday trip w...  \n",
       "3  AT_USER moodlighting is the only way to fly! b...  \n",
       "4  AT_USER plz help me win my bid upgrade for my ...  \n",
       "5  AT_USER - amazing customer service, again!   ğŸ’•...  \n",
       "6  AT_USER trying to book a flight &amp; your sit...  \n",
       "7  AT_USER my goodness your people AT_USER field ...  \n",
       "8                      AT_USER very nicely done.  ğŸ‘   \n",
       "9  AT_USER hahaha  ğŸ˜‚ AT_USER you guys are amazing...  "
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emoji[['clean_text', 'clean_emoji_text']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = df_emoji['clean_text'].values[0]\n",
    "text2 = df_emoji['clean_emoji_text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('i â¤ï¸ flying AT_USER â˜ºï¸ğŸ‘', 'i  â¤ ï¸ flying AT_USER  â˜º ï¸ ğŸ‘ ')"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1, text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['i', 'â¤ï¸', 'flying', 'AT_USER', 'â˜ºï¸ğŸ‘'],\n",
       " ['i', 'â¤', 'ï¸', 'flying', 'AT_USER', 'â˜º', 'ï¸', 'ğŸ‘'])"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.split(), text2.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```So now we have another column with modified emojis such that when the\n",
    "text is tokenized, emojis will appear as individual features.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit([text2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.set_params of CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)>"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.set_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at_user', 'flying']"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'at_user': 0, 'flying': 1}"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.findall(r'[^\\w\\s,]', a_list[0])\n",
    "# Out[75]: ['ğŸ¤”', 'ğŸ™ˆ', 'ğŸ˜Œ', 'ğŸ’•', 'ğŸ‘­', 'ğŸ‘™']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = df_emoji['clean_text'].values[0]\n",
    "text2 = df_emoji['clean_emoji_text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('i â¤ï¸ flying AT_USER â˜ºï¸ğŸ‘', 'i  â¤ ï¸ flying AT_USER  â˜º ï¸ ğŸ‘ ')"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1, text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['â¤', 'ï¸', 'â˜º', 'ï¸', 'ğŸ‘']"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'[^\\w\\s,]', text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['â¤', 'ï¸', 'â˜º', 'ï¸', 'ğŸ‘']"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'[^\\w\\s,]', text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flying', 'AT_USER']"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('(?u)\\\\b\\\\w\\\\w+\\\\b', text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'f', 'A']"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('(?u)\\\\b\\\\w', text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'flying', 'AT_USER']"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('(?u)\\\\b\\\\w+', text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['â¤ï¸', 'â˜ºï¸ğŸ‘']"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\S*[^\\w\\s]\\S*', text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', ' â¤ï¸', ' flying', ' AT_USER', ' â˜ºï¸ğŸ‘']"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[\\w+\\S*[^\\w+\\s]\\S*', text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', ' ', ' â¤', ' ï¸', ' flying', ' AT_USER', ' ', ' â˜º', ' ï¸', ' ğŸ‘', ' ']"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[\\w+\\S*[^\\w+\\s]\\S*', text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'hello ! & . this is !'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello    this is'"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = re.sub(\"[!.&]\", '', a).strip()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_emoji['clean_emoji_text'] = df_emoji['clean_text'].apply(lambda x: re.sub(\"[!.&-,]\", '',x).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ğŸ‘ğŸ‘âœˆï¸âœˆï¸ğŸ’—', 'when', 'are', 'you', 'guys', 'going', 'to', 'start', 'flying', 'to', 'paris?', 'AT_USER', 'AT_USER', 'youre', 'welcomeâ€']\n"
     ]
    }
   ],
   "source": [
    "print(df_emoji.clean_emoji_text[13].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¤\n",
      "â˜º\n",
      "ğŸ‘\n",
      "ğŸ˜¡\n",
      "ğŸ˜¢\n",
      "ğŸ’œ\n",
      "âœˆ\n",
      "ğŸ·\n",
      "ğŸ‘\n",
      "ğŸ’º\n",
      "âœˆ\n",
      "ğŸ’•\n",
      "ğŸ’•\n",
      "ğŸ˜\n",
      "â¤\n",
      "ğŸ‘\n",
      "ğŸ˜‚\n",
      "ğŸ’—\n",
      "ğŸ¸\n",
      "ğŸ˜’\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "âœˆ\n",
      "âœˆ\n",
      "ğŸ’—\n",
      "ğŸ˜Š\n",
      "ğŸ˜€\n",
      "ğŸ˜ƒ\n",
      "ğŸ˜„\n",
      "ğŸ˜\n",
      "ğŸ‘¸\n",
      "ğŸ’—\n",
      "ğŸ˜¥\n",
      "ğŸ€\n",
      "ğŸ€\n",
      "ğŸ€\n",
      "âœˆ\n",
      "ğŸ‰\n",
      "ğŸ’—\n",
      "ğŸ€\n",
      "ğŸ’—\n",
      "ğŸ˜ƒ\n",
      "ğŸ‘\n",
      "ğŸ˜¡\n",
      "ğŸ‘‹\n",
      "ğŸ˜¢\n",
      "âœŒ\n",
      "ğŸ™\n",
      "â¤\n",
      "ğŸ’œ\n",
      "ğŸ‘¿\n",
      "ğŸ˜‰\n",
      "ğŸ˜‰\n",
      "ğŸ˜”\n",
      "ğŸ˜­\n",
      "ğŸ˜Š\n",
      "âœˆ\n",
      "ğŸ‘\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "âœˆ\n",
      "ğŸ‘\n",
      "ğŸ†–\n",
      "ğŸ˜¢\n",
      "ğŸ’©\n",
      "âœ”\n",
      "ğŸŒ´\n",
      "ğŸŒ´\n",
      "âœ…\n",
      "âŒ\n",
      "ğŸ˜¡\n",
      "ğŸ˜Š\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ™\n",
      "ğŸ˜„\n",
      "ğŸ˜„\n",
      "ğŸ˜„\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜’\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ˜\n",
      "ğŸ˜¡\n",
      "ğŸ˜Š\n",
      "ğŸ‰\n",
      "ğŸ‰\n",
      "ğŸ‰\n",
      "ğŸ˜”\n",
      "ğŸ˜Š\n",
      "ğŸ˜­\n",
      "ğŸ‘\n",
      "ğŸ˜‰\n",
      "ğŸ˜¢\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ˜ˆ\n",
      "ğŸ˜¡\n",
      "ğŸ˜Š\n",
      "ğŸ‘\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜¤\n",
      "ğŸ˜¤\n",
      "ğŸ˜¤\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ˜Š\n",
      "â¤\n",
      "ğŸ‘\n",
      "ğŸ‘Œ\n",
      "ğŸ˜‚\n",
      "ğŸ˜€\n",
      "ğŸ‘Œ\n",
      "ğŸ’ª\n",
      "ğŸ˜‰\n",
      "ğŸ’”\n",
      "ğŸ˜ª\n",
      "ğŸ˜•\n",
      "ğŸ˜£\n",
      "ğŸ˜¬\n",
      "ğŸ‘\n",
      "ğŸ˜„\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ˜‹\n",
      "ğŸ™Œ\n",
      "ğŸ˜\n",
      "ğŸŒŸ\n",
      "ğŸŒŸ\n",
      "âœˆ\n",
      "âœˆ\n",
      "ğŸ“±\n",
      "ğŸ˜\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ»\n",
      "ğŸ»\n",
      "ğŸ‘\n",
      "ğŸ˜\n",
      "ğŸ˜¡\n",
      "ğŸ’–\n",
      "ğŸ˜”\n",
      "ğŸ˜”\n",
      "ğŸ˜”\n",
      "ğŸ˜¢\n",
      "ğŸ’\n",
      "ğŸ˜\n",
      "ğŸ˜œ\n",
      "ğŸ˜‚\n",
      "ğŸ˜·\n",
      "ğŸ˜±\n",
      "ğŸ˜\n",
      "ğŸ˜–\n",
      "â­\n",
      "âœˆ\n",
      "ğŸ‘\n",
      "ğŸ˜ƒ\n",
      "ğŸ’•\n",
      "ğŸµ\n",
      "âœˆ\n",
      "â—\n",
      "â¤\n",
      "ğŸ˜¤\n",
      "ğŸ´\n",
      "ğŸ˜¢\n",
      "âœˆ\n",
      "ğŸ˜\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ’”\n",
      "ğŸ’”\n",
      "ğŸ’”\n",
      "ğŸ’”\n",
      "ğŸ’”\n",
      "ğŸ’”\n",
      "ğŸ’”\n",
      "ğŸ˜\n",
      "ğŸ˜‰\n",
      "â¤\n",
      "ğŸ˜¥\n",
      "ğŸ˜†\n",
      "ğŸ˜Š\n",
      "ğŸŒ´\n",
      "â¤\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "âœˆ\n",
      "âœˆ\n",
      "âœˆ\n",
      "ğŸ˜ƒ\n",
      "ğŸ‘\n",
      "â¤\n",
      "â¤\n",
      "â¤\n",
      "ğŸ˜©\n",
      "ğŸ˜‘\n",
      "ğŸ’•\n",
      "â¤\n",
      "ğŸ˜ƒ\n",
      "ğŸ’•\n",
      "ğŸ˜\n",
      "â¤´\n",
      "â¤´\n",
      "ğŸ˜’\n",
      "ğŸ‘\n",
      "ğŸ˜œ\n",
      "â˜€\n",
      "ğŸ˜­\n",
      "â¤\n",
      "ğŸ‘Š\n",
      "ğŸ˜„\n",
      "ğŸ˜ƒ\n",
      "ğŸ˜€\n",
      "âœˆ\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ‰\n",
      "ğŸ‰\n",
      "ğŸ‰\n",
      "â¤\n",
      "ğŸ’¯\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ‘\n",
      "âœˆ\n",
      "ğŸ’©\n",
      "ğŸ’©\n",
      "ğŸ’©\n",
      "ğŸ’©\n",
      "ğŸ˜ \n",
      "ğŸ˜ \n",
      "ğŸ˜”\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜Š\n",
      "â˜•\n",
      "ğŸ“²\n",
      "âœˆ\n",
      "ğŸ˜ \n",
      "ğŸ‘º\n",
      "ğŸ™ˆ\n",
      "ğŸ’˜\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ˜\n",
      "ğŸ˜‚\n",
      "ğŸ‘‰\n",
      "ğŸšª\n",
      "ğŸ™…\n",
      "ğŸ˜‚\n",
      "ğŸ˜‚\n",
      "ğŸ˜‚\n",
      "ğŸ˜­\n",
      "ğŸ˜\n",
      "ğŸ˜†\n",
      "ğŸ˜µ\n",
      "ğŸ˜’\n",
      "âœˆ\n",
      "ğŸ”µ\n",
      "ğŸ”µ\n",
      "ğŸ”µ\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜‚\n",
      "ğŸ˜‚\n",
      "ğŸ˜‘\n",
      "ğŸ’™\n",
      "ğŸ‘€\n",
      "ğŸ‘€\n",
      "ğŸ˜’\n",
      "ğŸ˜‚\n",
      "ğŸ˜‚\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ‘€\n",
      "ğŸ˜\n",
      "ğŸ˜‘\n",
      "ğŸ˜Š\n",
      "ğŸ˜Š\n",
      "ğŸ’™\n",
      "ğŸ’™\n",
      "ğŸ’™\n",
      "ğŸ’™\n",
      "ğŸ˜‚\n",
      "ğŸ‘Œ\n",
      "ğŸ‘Œ\n",
      "ğŸ‘Œ\n",
      "ğŸ˜’\n",
      "â¤\n",
      "ğŸ˜­\n",
      "ğŸ˜’\n",
      "ğŸ˜\n",
      "ğŸ˜\n",
      "ğŸ˜\n",
      "ğŸ˜ƒ\n",
      "ğŸ˜‰\n",
      "â˜º\n",
      "âœˆ\n",
      "ğŸ™Œ\n",
      "âœˆ\n",
      "ğŸ˜•\n",
      "ğŸŒ´\n",
      "â˜º\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ˜Š\n",
      "ğŸ˜’\n",
      "ğŸ˜Š\n",
      "ğŸ˜Š\n",
      "ğŸ‘\n",
      "ğŸ˜\n",
      "ğŸ‰\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ†˜\n",
      "ğŸ˜˜\n",
      "ğŸ˜\n",
      "ğŸ˜¢\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ˜Š\n",
      "ğŸ˜­\n",
      "ğŸ’”\n",
      "ğŸ‘\n",
      "ğŸ˜·\n",
      "ğŸ˜‰\n",
      "â¤\n",
      "ğŸ˜\n",
      "ğŸ˜„\n",
      "ğŸ’™\n",
      "ğŸ’™\n",
      "ğŸ’™\n",
      "ğŸ˜ƒ\n",
      "ğŸ˜Š\n",
      "âœˆ\n",
      "ğŸ˜”\n",
      "â¤\n",
      "âœ¨\n",
      "ğŸ’™\n",
      "â˜•\n",
      "âœˆ\n",
      "ğŸ‘\n",
      "ğŸ˜¢\n",
      "ğŸ˜¢\n",
      "ğŸ·\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ˜Š\n",
      "ğŸ˜Š\n",
      "ğŸ˜Š\n",
      "âœˆ\n",
      "âœˆ\n",
      "âœˆ\n",
      "ğŸ˜¢\n",
      "ğŸ‘Œ\n",
      "â˜º\n",
      "ğŸ˜©\n",
      "ğŸŒ´\n",
      "ğŸ’™\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ’•\n",
      "âœˆ\n",
      "ğŸ’º\n",
      "âœˆ\n",
      "ğŸ˜”\n",
      "ğŸ˜“\n",
      "ğŸ˜¤\n",
      "â˜º\n",
      "ğŸ˜€\n",
      "ğŸ˜‰\n",
      "ğŸ˜‚\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "â¤\n",
      "ğŸ˜\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜Š\n",
      "ğŸ‘€\n",
      "ğŸ™\n",
      "ğŸ‘\n",
      "ğŸ˜‰\n",
      "ğŸ‘\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ˜’\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜Š\n",
      "âœˆ\n",
      "âŒš\n",
      "ğŸ˜‚\n",
      "ğŸ˜‚\n",
      "ğŸ˜‚\n",
      "ğŸ‘Š\n",
      "ğŸ˜³\n",
      "ğŸ˜¡\n",
      "âœŒ\n",
      "âœŒ\n",
      "ğŸ‘\n",
      "ğŸ˜©\n",
      "ğŸ˜­\n",
      "ğŸ˜­\n",
      "ğŸ˜ \n",
      "ğŸ˜‘\n",
      "ğŸ˜©\n",
      "ğŸ³\n",
      "ğŸ˜¡\n",
      "ğŸ‘Œ\n",
      "ğŸ˜¡\n",
      "ğŸ˜‚\n",
      "ğŸ’º\n",
      "âœˆ\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "âœŒ\n",
      "âœŒ\n",
      "âœŒ\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ˜Š\n",
      "â¤µ\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "â¤µ\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "âœŒ\n",
      "âœŒ\n",
      "âœŒ\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ‘\n",
      "âœˆ\n",
      "ğŸ‘\n",
      "ğŸ˜¬\n",
      "ğŸ‘\n",
      "ğŸ˜¡\n",
      "ğŸ˜Š\n",
      "ğŸ‘\n",
      "ğŸ˜¤\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "âœŒ\n",
      "âœŒ\n",
      "âœŒ\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ‘¿\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "âœŒ\n",
      "âœŒ\n",
      "âœŒ\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ‘ \n",
      "ğŸ‘ \n",
      "ğŸ‘ \n",
      "ğŸ˜‘\n",
      "ğŸ˜‰\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ˜¢\n",
      "ğŸ˜¢\n",
      "ğŸ˜¢\n",
      "ğŸ˜¢\n",
      "ğŸ˜¢\n",
      "ğŸ˜¢\n",
      "ğŸ˜¢\n",
      "ğŸ˜¢\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ˜¢\n",
      "ğŸ˜¢\n",
      "ğŸ˜¢\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "ğŸ™\n",
      "âœŒ\n",
      "ğŸ˜‰\n",
      "âœˆ\n",
      "ğŸŒ\n",
      "ğŸ˜•\n",
      "ğŸ˜ƒ\n",
      "ğŸ˜„\n",
      "ğŸ˜\n",
      "ğŸ˜„\n",
      "ğŸ˜\n",
      "ğŸ˜’\n",
      "ğŸ˜„\n",
      "ğŸ™\n",
      "ğŸ˜­\n",
      "ğŸ˜€\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "ğŸ˜\n",
      "ğŸ‘\n",
      "ğŸ˜¡\n",
      "âœˆ\n",
      "ğŸ‘Œ\n",
      "ğŸ‘Œ\n",
      "ğŸ‘Œ\n",
      "ğŸ‘€\n",
      "ğŸ˜\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ‘\n",
      "ğŸ˜¡\n",
      "ğŸ˜¡\n",
      "âœˆ\n",
      "ğŸ˜¥\n",
      "ğŸ˜¥\n",
      "âœˆ\n",
      "ğŸ˜‹\n",
      "ğŸ˜¢\n",
      "âœˆ\n",
      "ğŸ˜­\n",
      "ğŸ™\n",
      "ğŸ˜„\n",
      "ğŸ’\n",
      "ğŸ’\n",
      "ğŸ’\n",
      "ğŸ˜¤\n",
      "ğŸ‘\n",
      "ğŸ˜¡\n",
      "ğŸ˜¤\n",
      "ğŸ˜–\n",
      "ğŸ˜²\n",
      "ğŸ˜©\n",
      "ğŸ˜¢\n",
      "ğŸ˜•\n",
      "ğŸ˜¦\n",
      "ğŸ˜³\n",
      "ğŸ˜’\n",
      "ğŸ‘\n",
      "ğŸ˜¡\n",
      "â¤\n",
      "â¤\n",
      "ğŸ˜’\n",
      "ğŸ˜ƒ\n",
      "ğŸ˜“\n",
      "ğŸ˜­\n",
      "âœˆ\n",
      "â¡\n",
      "ğŸ˜•\n",
      "ğŸ˜ƒ\n",
      "âœˆ\n",
      "âœˆ\n",
      "âœˆ\n",
      "âœˆ\n",
      "âœˆ\n",
      "âœˆ\n",
      "âœˆ\n",
      "âœˆ\n",
      "âœŒ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_emoji['clean_emoji_text'] = df_emoji['clean_emoji_text'].apply(individualize_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ğŸ‘', 'ğŸ‘', 'âœˆ', 'ï¸', 'âœˆ', 'ï¸', 'ğŸ’—', 'when', 'are', 'you', 'guys', 'going', 'to', 'start', 'flying', 'to', 'paris?', 'AT_USER', 'AT_USER', 'youre', 'welcomeâ€']\n"
     ]
    }
   ],
   "source": [
    "print(df_emoji.clean_emoji_text[13].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see what CountVectorizer does with the emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following regex tells cv to tokenize words and symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(token_pattern='[\\w+\\S*[^\\w+\\s]\\S*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = df_emoji.clean_text[13]\n",
    "text2 = df_emoji.clean_emoji_text[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='[\\\\w+\\\\S*[^\\\\w+\\\\s]\\\\S*',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit([text1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' are', ' at_user', ' flying', ' going', ' guys', ' paris?', ' start', ' to', ' welcome.â€', ' when', ' you', \" you're\", 'ğŸ‘ğŸ‘âœˆï¸âœˆï¸ğŸ’—']\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='[\\\\w+\\\\S*[^\\\\w+\\\\s]\\\\S*',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit([text2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' are', ' at_user', ' flying', ' going', ' guys', ' paris?', ' start', ' to', ' welcomeâ€', ' when', ' you', ' youre', ' âœˆ', ' ï¸', ' ğŸ‘', ' ğŸ’—']\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_emoji_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i  â¤ ï¸ flying AT_USER  â˜º ï¸ ğŸ‘</td>\n",
       "      <td>i â¤ï¸ flying AT_USER â˜ºï¸ğŸ‘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AT_USER you guys messed up my seating i reserv...</td>\n",
       "      <td>AT_USER you guys messed up my seating.. i rese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AT_USER hi i just bked a cool birthday trip wi...</td>\n",
       "      <td>AT_USER hi! i just bked a cool birthday trip w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AT_USER moodlighting is the only way to fly be...</td>\n",
       "      <td>AT_USER moodlighting is the only way to fly! b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AT_USER plz help me win my bid upgrade for my ...</td>\n",
       "      <td>AT_USER plz help me win my bid upgrade for my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AT_USER - amazing customer service again   ğŸ’•  ...</td>\n",
       "      <td>AT_USER - amazing customer service, again! ğŸ’•ğŸ’• ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AT_USER trying to book a flight amp; your site...</td>\n",
       "      <td>AT_USER trying to book a flight &amp;amp; your sit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AT_USER my goodness your people AT_USER field ...</td>\n",
       "      <td>AT_USER my goodness your people AT_USER field ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AT_USER very nicely done  ğŸ‘</td>\n",
       "      <td>AT_USER very nicely done. ğŸ‘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AT_USER hahaha  ğŸ˜‚ AT_USER you guys are amazing...</td>\n",
       "      <td>AT_USER hahaha ğŸ˜‚AT_USER you guys are amazing. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AT_USER thanks good times there and back vodka...</td>\n",
       "      <td>AT_USER thanks! good times there and back! vod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AT_USER requested window seat and confirmed wi...</td>\n",
       "      <td>AT_USER requested window seat and confirmed wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AT_USER moved my seat with no notice \"better s...</td>\n",
       "      <td>AT_USER moved my seat with no notice. \"better ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ğŸ‘    ğŸ‘    âœˆ  ï¸  âœˆ  ï¸ ğŸ’—  when are you guys go...</td>\n",
       "      <td>ğŸ‘ğŸ‘âœˆï¸âœˆï¸ğŸ’— when are you guys going to start flyin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AT_USER need to start flying to AT_USER   ğŸ˜Š  ğŸ˜€...</td>\n",
       "      <td>AT_USER need to start flying to AT_USER . ğŸ˜ŠğŸ˜€ğŸ˜ƒğŸ˜„</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     clean_emoji_text  \\\n",
       "0                       i  â¤ ï¸ flying AT_USER  â˜º ï¸ ğŸ‘    \n",
       "1   AT_USER you guys messed up my seating i reserv...   \n",
       "2   AT_USER hi i just bked a cool birthday trip wi...   \n",
       "3   AT_USER moodlighting is the only way to fly be...   \n",
       "4   AT_USER plz help me win my bid upgrade for my ...   \n",
       "5   AT_USER - amazing customer service again   ğŸ’•  ...   \n",
       "6   AT_USER trying to book a flight amp; your site...   \n",
       "7   AT_USER my goodness your people AT_USER field ...   \n",
       "8                        AT_USER very nicely done  ğŸ‘    \n",
       "9   AT_USER hahaha  ğŸ˜‚ AT_USER you guys are amazing...   \n",
       "10  AT_USER thanks good times there and back vodka...   \n",
       "11  AT_USER requested window seat and confirmed wi...   \n",
       "12  AT_USER moved my seat with no notice \"better s...   \n",
       "13    ğŸ‘    ğŸ‘    âœˆ  ï¸  âœˆ  ï¸ ğŸ’—  when are you guys go...   \n",
       "14  AT_USER need to start flying to AT_USER   ğŸ˜Š  ğŸ˜€...   \n",
       "\n",
       "                                           clean_text  \n",
       "0                             i â¤ï¸ flying AT_USER â˜ºï¸ğŸ‘  \n",
       "1   AT_USER you guys messed up my seating.. i rese...  \n",
       "2   AT_USER hi! i just bked a cool birthday trip w...  \n",
       "3   AT_USER moodlighting is the only way to fly! b...  \n",
       "4   AT_USER plz help me win my bid upgrade for my ...  \n",
       "5   AT_USER - amazing customer service, again! ğŸ’•ğŸ’• ...  \n",
       "6   AT_USER trying to book a flight &amp; your sit...  \n",
       "7   AT_USER my goodness your people AT_USER field ...  \n",
       "8                         AT_USER very nicely done. ğŸ‘  \n",
       "9   AT_USER hahaha ğŸ˜‚AT_USER you guys are amazing. ...  \n",
       "10  AT_USER thanks! good times there and back! vod...  \n",
       "11  AT_USER requested window seat and confirmed wi...  \n",
       "12  AT_USER moved my seat with no notice. \"better ...  \n",
       "13  ğŸ‘ğŸ‘âœˆï¸âœˆï¸ğŸ’— when are you guys going to start flyin...  \n",
       "14     AT_USER need to start flying to AT_USER . ğŸ˜ŠğŸ˜€ğŸ˜ƒğŸ˜„  "
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emoji[['clean_emoji_text', 'clean_text']][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='[\\\\w+\\\\S*[^\\\\w+\\\\s]\\\\S*',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(df_emoji['clean_emoji_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' \"bad', ' \"better', ' \"fortunately\"?', ' $100', ' $159', ' $20', ' $250', ' $73', ' -', ' /', ' /dying', ' 0%', ' 0/3', ' 0671', ' 1', ' 1/2', ' 10', ' 11:31', ' 127', ' 12:08', ' 1384', ' 1535', ' 1583', ' 15minutes', ' 16', ' 1898', ' 1hour', ' 1k', ' 1st', ' 2', ' 2/17', ' 2/18?', ' 2/27', ' 20', ' 226', ' 27', ' 277', ' 28hrs', ' 2day', ' 2hrs', ' 2nd', ' 2y', ' 3', ' 30', ' 32', ' 34', ' 3403', ' 3589', ' 37000ft', ' 384', ' 3a', ' 3ticketsforjax', ' 4', ' 4/24/15', ' 40', ' 409', ' 4229', ' 423', ' 4251', ' 4487', ' 45', ' 4649', ' 5', ' 51', ' 5lbs', ' 6', ' 654', ' 6th', ' 7', ' 700', ' 729', ' 738', ' 79$', ' 8:30', ' 8hr', ' 9', ' :', ' :-', ' :â€', ' ;â€', ' =', ' ?', ' @', ' ^^', ' ^eyâ€', ' ^llâ€', ' a', ' aa', ' aa106', ' aa45', ' able', ' about', ' above', ' absolutely', ' access', ' ad', ' add', ' additional', ' adjusting', ' affected', ' after', ' again', ' agent', ' agents', ' agianand', ' ago', ' agree', ' ahah', ' ahead', ' air', ' airfare', ' airline', ' airlines', ' airplane', ' airplanes', ' airport', ' airways', ' aka', ' al', ' alaska', ' all', ' allergy', ' almost', ' alone', ' along', ' already', ' also', ' always', ' always-', ' alwayslate', ' am', ' amazed', ' amazing', ' america', ' american', ' americanairlines', ' among', ' amp;', ' amp;amp;', ' an', ' and', ' angry', ' annoying', ' another', ' answer', ' answered', ' anthony', ' any', ' anyone', ' anything', ' apologize', ' apology', ' app', ' apparently', ' applied', ' appreciate', ' are', ' arent', ' arkansas', ' armrest', ' arrival', ' arrive', ' arriving', ' as', ' asked', ' assistanceyou', ' at', ' at_user', ' atlanta', ' attendant', ' attendants', ' attendants:', ' automated', ' away', ' awesome', ' az', ' b', ' back', ' back:', ' back;', ' back?', ' bad', ' bae', ' bag', ' baggage', ' bags', ' ball', ' barbara', ' based', ' battling', ' bc', ' bday', ' bdl', ' be', ' beatstheothers', ' beauty', ' because', ' beefjerky', ' been', ' beer', ' before', ' before/after', ' behavior', ' behind', ' being', ' best', ' bethonors', ' better', ' between', ' beverages', ' beyond', ' bid', ' big', ' biggest', ' biggie', ' birthday', ' bked', ' blue', ' bluecarpet', ' bna', ' board', ' boarded', ' boarding', ' boarding?', ' book', ' booked', ' booking', ' books', ' bos', ' bos-sf', ' both', ' bottom', ' bowl', ' boy', ' brag', ' brand', ' bravo', ' break', ' bring', ' bringin', ' broke', ' broken', ' bs', ' bumped', ' bumper', ' bur', ' business', ' but', ' bwi', ' by', ' bze', ' c11', ' cabin', ' cache?', ' call', ' called', ' calming', ' came', ' can', ' cancelled', ' cancer', ' cancun', ' cannot', ' cant', ' card', ' care', ' caring', ' carrying', ' catch', ' cats', ' catsanddogslivingtogether', ' cause', ' center', ' challenges', ' changes', ' channels', ' chaos', ' cheaper', ' check', ' check-ins', ' checked', ' checking', ' checkpoint', ' cheers', ' chicago', ' chicagothanks', ' childs', ' chill', ' choosing', ' circumstances', ' claim', ' clarence', ' class', ' class/write', ' cle', ' clean', ' cleaned', ' cleaned???', ' clearly', ' clever', ' click', ' clicks', ' clt', ' cmh', ' cmon', ' code', ' coffeeat', ' cold', ' college', ' come', ' comfortable', ' coming', ' communication', ' commute', ' company', ' compassionate', ' competitors', ' complain', ' complete', ' computer', ' concentrate', ' concerned', ' conditions', ' conection', ' confirmation', ' confirmed', ' conflicting', ' confused', ' confusing', ' congrats', ' connecting', ' connection', ' connections', ' consecutive', ' consideration', ' contain', ' continues', ' control', ' controller', ' conv?', ' cool', ' cot', ' could', ' couldnt', ' country', ' course', ' courteous', ' cranky', ' credit', ' crew', ' crews', ' crisis', ' crying', ' cust', ' customer', ' customerits', ' customers', ' customerservice', ' cxl', ' dallas', ' damaged', ' damn', ' david', ' day', ' days', ' daytona', ' dc', ' dca', ' ddean', ' de', ' de-ice', ' de-icing', ' deal', ' deane', ' decide', ' decision', ' def', ' delacy', ' delay', ' delayed', ' delaying', ' delays', ' delays?', ' deleted', ' departure', ' despite', ' destinationdragons', ' did', ' didnt', ' diego?', ' different', ' direct', ' dirty', ' disappointed', ' disconnected', ' disconnects', ' discount', ' discover', ' dishonoring', ' div', ' diverted', ' dividends', ' dm', ' do', ' does', ' doesnt', ' dog', ' doing', ' dollars', ' done', ' done?', ' dont', ' door', ' down', ' dragons', ' draw', ' dre', ' dread', ' drink', ' drive', ' dropped', ' dude', ' due', ' during', ' each', ' earlier', ' earth', ' easier', ' effort', ' eh', ' either', ' elevate', ' else', ' elsewhere', ' email', ' employees', ' enhance', ' enjoy', ' enough', ' entered', ' entire', ' enzo', ' epicfail', ' especially', ' even', ' ever', ' every', ' everywhere', ' ewr', ' ewr-iad', ' example', ' excellent', ' excited', ' exciting', ' exp', ' experience', ' explanation', ' extra', ' extreme', ' extremely', ' fa', ' faaaannntastic', ' fabulous', ' fail', ' failing', ' fails', ' failure', ' fam', ' families', ' family', ' far', ' fault', ' feb', ' fee', ' feed', ' feel', ' fees', ' feesbut', ' feet', ' festivities', ' few', ' fidencio', ' field', ' filled', ' finally', ' find', ' finding', ' first', ' five', ' fix', ' fleek', ' fleets', ' flew', ' flexibility/compassion', ' flight', ' flight/get', ' flightations:', ' flighted', ' flightlation', ' flightled', ' flightled?', ' flightly', ' flightr', ' flights', ' flights?', ' flightst', ' fll', ' fllgt;sfo', ' floor', ' florida', ' flown', ' flt', ' fly', ' fly=130$', ' flyers', ' flying', ' follow', ' followed', ' following', ' food', ' for', ' forever', ' forgive', ' forgiven', ' forgotmiahe', ' forward', ' fran', ' frank', ' free', ' freeneversucks', ' freezing', ' frequent', ' freyasfund', ' friday', ' friend', ' friends', ' from', ' frustrated', ' ft', ' ftw', ' fuck', ' fuckin', ' fudgin', ' funeral', ' funny', ' future', ' gainesville', ' gate', ' gave', ' get', ' gets', ' getting', ' gettingbetter', ' give', ' given', ' giving', ' glad', ' gmail', ' go', ' go?', ' goal:', ' goes', ' going', ' gold', ' gonna', ' good', ' goodness', ' gorgeous', ' got', ' gotcha', ' gotta', ' gotten', ' grades', ' grateful', ' great', ' greatest', ' ground', ' group', ' guess', ' guy', ' guys', ' had', ' haha', ' hahah', ' hahaha', ' half', ' handle', ' hang', ' happens', ' happy', ' happytweet', ' hard', ' has', ' hasnt', ' hasshe', ' hate', ' have', ' havent', ' having', ' hawaii', ' he', ' headed', ' hear', ' heard', ' heart', ' heel', ' hello', ' help', ' help--faith', ' helped', ' helpful', ' helpfulness', ' helping', ' her', ' here', ' herself', ' hey', ' hi', ' highlight', ' him', ' his', ' hold', ' home', ' homegirl', ' honey', ' honor', ' hoo', ' hoot', ' hope', ' hoping', ' hoping/praying', ' horse', ' hospitality', ' hot', ' hotel', ' hour', ' hours', ' hours?', ' how', ' hr', ' hrs', ' humphrey', ' hung', ' hungry', ' hurt', ' i', ' iad', ' id', ' idols', ' if', ' ignoring', ' ihop', ' ill', ' im', ' imagine', ' imjustsaying', ' impossible', ' impressed', ' imtired', ' in', ' in-flight', ' incredible', ' industry', ' info', ' instead', ' integration', ' internet', ' into', ' inventory', ' inğŸ‡ºğŸ‡¸2y', ' iphone', ' irreplaceable', ' is', ' isnt', ' issue', ' issues', ' it', ' it?', ' its', ' itâ€™ll', ' ive', ' jacksonville', ' jb', ' jbi', ' jerks', ' jet', ' jetblue', ' jetbluemember', ' jfk', ' job', ' joke', ' joke?', ' juan', ' jumped', ' just', ' justdoit', ' justwantmybed', ' keep', ' keepitup', ' kept', ' kidding', ' kids', ' kind', ' kindness', ' kit', ' know', ' la', ' land', ' landed', ' landing', ' las', ' las?', ' last', ' lastella', ' late', ' lauderdale', ' lax', ' lax---gt;sea', ' least', ' leave', ' leaving', ' leavingtomm', ' left', ' legroom', ' lemme', ' let', ' letting', ' lga', ' life', ' like', ' line', ' link', ' little', ' load', ' lol', ' lol?', ' long', ' look', ' looking', ' looks', ' looong', ' lost', ' lost/stolen', ' lots', ' louisville', ' love', ' loved', ' lovely', ' loving', ' loyalty', ' lt;lt;now', ' lucky', ' luggage', ' luv', ' mad', ' made', ' maimi', ' maimi???', ' main', ' maintained', ' maintenance', ' major', ' make', ' makes', ' making', ' manchester', ' many', ' marks', ' match', ' math', ' may', ' maybe', ' mci', ' mco?', ' mdwgt;mem', ' me', ' mean', ' means', ' meanâ€¦it', ' mech', ' meet', ' meeting', ' meetingoutraged', ' meetings', ' member', ' memorial', ' memories', ' mentioned', ' messed', ' mia-ewr', ' miami', ' middle', ' midterm', ' mileage', ' miles', ' milestone', ' military', ' min', ' mins', ' minute', ' minutes', ' miss', ' missed', ' missing', ' mistake', ' mistakes', ' moodlighting', ' moodlitmonday', ' more', ' mosaic', ' most', ' moved', ' much', ' muh', ' must', ' my', ' n', ' name', ' name?', ' named', ' natural', ' nc', ' need', ' needed', ' needs', ' nerves', ' never', ' neverflyusairways', ' new', ' newark', ' news', ' next', ' nice', ' nicely', ' nicest', ' night', ' nj', ' no', ' nonstop', ' noooo', ' norma', ' normally', ' not', ' not?', ' notahappycustomer', ' nothanks', ' nothappy', ' nothing', ' notice', ' notifying', ' now', ' number', ' ny', ' ny-ers', ' nyc', ' of', ' off', ' offer', ' offers', ' official', ' officially', ' often', ' oh', ' ohare', ' ok', ' ok?', ' old', ' omaha?', ' on', ' on-time', ' onboard', ' once', ' one', ' ones', ' onfleek', ' only', ' onto', ' open', ' opened', ' opens', ' opposed', ' options', ' or', ' ord', ' ord--gt;cdg', ' order', ' orlando', ' oscar', ' other', ' ots', ' our', ' ourprincess', ' out', ' outfitted', ' outstanding', ' over', ' p', ' page', ' pants?', ' paris?', ' part', ' party', ' pass', ' passbook', ' passed', ' passengers', ' past', ' patient', ' pay', ' peanut', ' peanuts', ' peculiar', ' people', ' perks', ' person', ' philadelphia', ' philly', ' phl', ' phoenix', ' phone', ' photos', ' phx', ' pick', ' picked', ' pics', ' picture', ' pilot', ' place', ' plane', ' plane?', ' planes', ' planned', ' plans', ' plat', ' play', ' please', ' please;', ' pleasure', ' pls', ' plus', ' plz', ' point', ' points', ' pointsâ€', ' pooch', ' poor', ' positive', ' pound', ' pr', ' preciation', ' prefer', ' preferably', ' pressure', ' previously', ' price', ' pricing', ' priorities', ' priority', ' prize???', ' probably', ' problem', ' problems', ' procedure', ' professional', ' professors', ' promos', ' properly', ' props', ' prove', ' provided', ' puerto', ' purchase', ' purifier', ' purpose?', ' put', ' putting', ' qualify', ' question', ' races', ' radish', ' raeann', ' raise', ' rather', ' read', ' real', ' really', ' reason', ' rebooked', ' receiving', ' record', ' redsox', ' reflight', ' refreshed', ' refund', ' refused', ' reimbursement', ' reinstated', ' relations', ' relaxing', ' reminder', ' rental', ' rep', ' repaid', ' replacing', ' reply', ' reply?', ' reps', ' requested', ' requirement', ' reservation', ' reservations', ' reserved', ' response', ' restored', ' rethinking', ' return', ' rico', ' ridiculous', ' right', ' ripme', ' rn', ' roasted', ' roasted?', ' rock', ' round-trip', ' route', ' row', ' rt', ' rtâ€œat_user', ' rude', ' rudeness', ' runway', ' ruth', ' sac', ' sad', ' safe', ' safety', ' said', ' salted', ' same', ' san', ' sanitized?', ' sass', ' sat', ' saved', ' say', ' says', ' sba?', ' school', ' scott', ' sea', ' seat', ' seat\"', ' seatac', ' seating', ' seats', ' seats/tray', ' seattlebound', ' second', ' security', ' sedholm', ' see', ' seem', ' seen', ' select', ' selected', ' semester', ' send', ' sending', ' sense', ' sent', ' serv', ' service', ' service:', ' services', ' set', ' several', ' sf', ' sfo', ' share', ' sharing', ' she', ' shes', ' shhhh', ' shortage', ' should', ' shouldnt', ' shouldve', ' show', ' shut', ' sick', ' side', ' sign', ' silver', ' sincerely', ' sing', ' sister', ' site', ' sitting', ' situation', ' sjc', ' sleeping', ' slow', ' smooth', ' snacks', ' snow', ' snowstorms', ' so', ' sold', ' some', ' something', ' song', ' soon', ' sooner', ' sooo', ' sooooo', ' sounds', ' southbendinwhere', ' southwest', ' southwestairlines', ' speed', ' spoke', ' squished', ' staff', ' standing', ' start', ' started', ' starting', ' stat', ' static', ' status', ' steal', ' steps', ' still', ' stl', ' stop', ' storm', ' strand', ' stranded', ' streamline', ' stuck', ' stuckintampa', ' student', ' such', ' suck', ' suitcase', ' sunday', ' super', ' support', ' supposed', ' suprise', ' sure', ' survive', ' suspended', ' sw', ' sweet', ' swooped', ' system', ' t-minus', ' t5', ' tables', ' take', ' taken', ' taking', ' talked', ' talking', ' tallahassee:', ' taxi', ' tea', ' team', ' technical', ' tell', ' telling', ' ten', ' terminal', ' terrific', ' test', ' texas', ' texts', ' than', ' thank', ' thanks', ' thankyou', ' that', ' thats', ' the', ' their', ' them', ' then', ' there', ' theres', ' these', ' theworst', ' they', ' theyll', ' theyve', ' thing', ' things', ' think', ' this', ' this?', ' tho', ' those', ' though', ' thought', ' three', ' through', ' throughout', ' throw', ' throwing', ' thru', ' thx', ' tick', ' ticket', ' ticketits', ' tickets', ' time', ' time?', ' timeliness:', ' timely', ' times', ' tix', ' tke', ' to', ' today', ' toilet', ' told', ' tomoro', ' tomorrow', ' tonight', ' too', ' took', ' top', ' toss', ' totally', ' touched', ' town', ' traffic', ' train', ' transported', ' travel', ' traveling', ' travellers', ' treat', ' treated', ' treatment', ' tried', ' trip', ' trip:', ' trips', ' truck', ' truebluepoints', ' truly', ' trying', ' tsa??', ' turned', ' tv', ' tweet', ' tweeted', ' tweeter', ' tweeting', ' twice', ' twitter', ' two', ' u', ' unacceptable', ' unaccommodating', ' unbelievable', ' under', ' understands', ' unfortunately', ' unhelpful', ' united', ' university', ' unpredictable', ' until', ' up', ' update', ' upgrade', ' upgraded', ' uppp', ' ups', ' upsmh', ' ur', ' urgent', ' url', ' us', ' usa', ' use', ' used', ' useful', ' useless', ' user', ' vaca', ' vacation', ' vacations', ' valued', ' vanity', ' vegas', ' very', ' via', ' video', ' vino', ' virgin', ' virginamerica', ' vodkatonics', ' vote', ' voucher', ' vx', ' w', ' w/', ' w/aa', ' wait', ' waited', ' waiting', ' waived', ' waivers', ' wanna', ' wannagohome', ' want', ' wanting', ' warm', ' was', ' wasnt', ' wasteoftime', ' watching', ' way', ' we', ' weak', ' weather', ' weather\"?', ' wed', ' wedding', ' week', ' week?', ' weekend', ' weekends', ' welcomeâ€', ' welcoming', ' well', ' wendi', ' went', ' were', ' weve', ' what', ' whats', ' when', ' where', ' whether', ' while', ' who', ' whoot', ' wht', ' why', ' why??', ' wifi', ' will', ' willing', ' win', ' window', ' wins', ' wish', ' wit', ' with', ' without', ' won', ' wonder', ' wonderful', ' wondering', ' wont', ' woo', ' word', ' work', ' worked', ' working', ' worries', ' worse', ' worst', ' would', ' wouldnt', ' wow', ' wrench', ' write?', ' ya', ' yall', ' year', ' years', ' yelled', ' yep', ' yes', ' yeseniahernandez', ' yet', ' yo', ' you', ' youdo', ' youknowyouwantto', ' your', ' youre', ' yours', ' yr', ' â€œat_user', ' âŒš', ' â˜€', ' â˜•', ' â˜º', ' âœ…', ' âœˆ', ' âœŒ', ' âœ”', ' âœ¨', ' âŒ', ' â—', ' â¤', ' â¡', ' â¤´', ' â¤µ', ' â­', ' ï¸', ' ï¸8c', ' ï¸amp;', ' ï¸anywhere', ' ï¸bos', ' ï¸from', ' ï¸ny', ' ï¸out', ' ï¸tampa', ' ğŸ†–', ' ğŸ†˜', ' ğŸ‡¬ğŸ‡§', ' ğŸ‡ºğŸ‡¸', ' ğŸŒ', ' ğŸŒŸ', ' ğŸŒ´', ' ğŸ·', ' ğŸ¸', ' ğŸ»', ' ğŸ€', ' ğŸ‰', ' ğŸµ', ' ğŸ³', ' ğŸ´', ' ğŸ‘€', ' ğŸ‘‰', ' ğŸ‘Š', ' ğŸ‘‹', ' ğŸ‘Œ', ' ğŸ‘', ' ğŸ‘', ' ğŸ‘', ' ğŸ‘ ', ' ğŸ‘¸', ' ğŸ‘º', ' ğŸ‘¿', ' ğŸ’”', ' ğŸ’•', ' ğŸ’–', ' ğŸ’—', ' ğŸ’˜', ' ğŸ’™', ' ğŸ’œ', ' ğŸ’', ' ğŸ’©', ' ğŸ’ª', ' ğŸ’¯', ' ğŸ’º', ' ğŸ“±', ' ğŸ“²', ' ğŸ”µ', ' ğŸ˜€', ' ğŸ˜', ' ğŸ˜‚', ' ğŸ˜ƒ', ' ğŸ˜„', ' ğŸ˜†', ' ğŸ˜ˆ', ' ğŸ˜‰', ' ğŸ˜Š', ' ğŸ˜‹', ' ğŸ˜', ' ğŸ˜', ' ğŸ˜', ' ğŸ˜', ' ğŸ˜‘', ' ğŸ˜’', ' ğŸ˜“', ' ğŸ˜”', ' ğŸ˜•', ' ğŸ˜–', ' ğŸ˜˜', ' ğŸ˜œ', ' ğŸ˜', ' ğŸ˜ ', ' ğŸ˜¡', ' ğŸ˜¢', ' ğŸ˜£', ' ğŸ˜¤', ' ğŸ˜¥', ' ğŸ˜¦', ' ğŸ˜©', ' ğŸ˜ª', ' ğŸ˜¬', ' ğŸ˜­', ' ğŸ˜±', ' ğŸ˜²', ' ğŸ˜³', ' ğŸ˜µ', ' ğŸ˜·', ' ğŸ™…', ' ğŸ™ˆ', ' ğŸ™Œ', ' ğŸ™', ' ğŸšª', 'at_user', 'his', 'i', 'is', 'omgee', 'really', 'rt', 'stahp', 'thank', 'why?', 'yes', 'â€œat_user'] 1525\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names(), len(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1525"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1 = CountVectorizer()\n",
    "cv1.fit(df_emoji['clean_emoji_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0671', '08', '10', '100', '11', '12', '127', '130', '1384', '15', '1535', '1583', '159', '15minutes', '16', '17', '18', '1898', '1hour', '1k', '1st', '20', '226', '24', '250', '27', '277', '28hrs', '2day', '2hrs', '2nd', '2y', '30', '31', '32', '34', '3403', '3589', '37000ft', '384', '3a', '3ticketsforjax', '40', '409', '4229', '423', '4251', '4487', '45', '4649', '51', '5lbs', '654', '6th', '700', '729', '73', '738', '79', '8c', '8hr', 'aa', 'aa106', 'aa45', 'able', 'about', 'above', 'absolutely', 'access', 'ad', 'add', 'additional', 'adjusting', 'affected', 'after', 'again', 'agent', 'agents', 'agianand', 'ago', 'agree', 'ahah', 'ahead', 'air', 'airfare', 'airline', 'airlines', 'airplane', 'airplanes', 'airport', 'airways', 'aka', 'al', 'alaska', 'all', 'allergy', 'almost', 'alone', 'along', 'already', 'also', 'always', 'alwayslate', 'am', 'amazed', 'amazing', 'america', 'american', 'americanairlines', 'among', 'amp', 'an', 'and', 'angry', 'annoying', 'another', 'answer', 'answered', 'anthony', 'any', 'anyone', 'anything', 'anywhere', 'apologize', 'apology', 'app', 'apparently', 'applied', 'appreciate', 'are', 'arent', 'arkansas', 'armrest', 'arrival', 'arrive', 'arriving', 'as', 'asked', 'assistanceyou', 'at', 'at_user', 'atlanta', 'attendant', 'attendants', 'automated', 'away', 'awesome', 'az', 'back', 'bad', 'bae', 'bag', 'baggage', 'bags', 'ball', 'barbara', 'based', 'battling', 'bc', 'bday', 'bdl', 'be', 'beatstheothers', 'beauty', 'because', 'beefjerky', 'been', 'beer', 'before', 'behavior', 'behind', 'being', 'best', 'bethonors', 'better', 'between', 'beverages', 'beyond', 'bid', 'big', 'biggest', 'biggie', 'birthday', 'bked', 'blue', 'bluecarpet', 'bna', 'board', 'boarded', 'boarding', 'book', 'booked', 'booking', 'books', 'bos', 'both', 'bottom', 'bowl', 'boy', 'brag', 'brand', 'bravo', 'break', 'bring', 'bringin', 'broke', 'broken', 'bs', 'bumped', 'bumper', 'bur', 'business', 'but', 'bwi', 'by', 'bze', 'c11', 'cabin', 'cache', 'call', 'called', 'calming', 'came', 'can', 'cancelled', 'cancer', 'cancun', 'cannot', 'cant', 'card', 'care', 'caring', 'carrying', 'catch', 'cats', 'catsanddogslivingtogether', 'cause', 'cdg', 'center', 'challenges', 'changes', 'channels', 'chaos', 'cheaper', 'check', 'checked', 'checking', 'checkpoint', 'cheers', 'chicago', 'chicagothanks', 'childs', 'chill', 'choosing', 'circumstances', 'claim', 'clarence', 'class', 'cle', 'clean', 'cleaned', 'clearly', 'clever', 'click', 'clicks', 'clt', 'cmh', 'cmon', 'code', 'coffeeat', 'cold', 'college', 'come', 'comfortable', 'coming', 'communication', 'commute', 'company', 'compassion', 'compassionate', 'competitors', 'complain', 'complete', 'computer', 'concentrate', 'concerned', 'conditions', 'conection', 'confirmation', 'confirmed', 'conflicting', 'confused', 'confusing', 'congrats', 'connecting', 'connection', 'connections', 'consecutive', 'consideration', 'contain', 'continues', 'control', 'controller', 'conv', 'cool', 'cot', 'could', 'couldnt', 'country', 'course', 'courteous', 'cranky', 'credit', 'crew', 'crews', 'crisis', 'crying', 'cust', 'customer', 'customerits', 'customers', 'customerservice', 'cxl', 'dallas', 'damaged', 'damn', 'david', 'day', 'days', 'daytona', 'dc', 'dca', 'ddean', 'de', 'deal', 'deane', 'decide', 'decision', 'def', 'delacy', 'delay', 'delayed', 'delaying', 'delays', 'deleted', 'departure', 'despite', 'destinationdragons', 'did', 'didnt', 'diego', 'different', 'direct', 'dirty', 'disappointed', 'disconnected', 'disconnects', 'discount', 'discover', 'dishonoring', 'div', 'diverted', 'dividends', 'dm', 'do', 'does', 'doesnt', 'dog', 'doing', 'dollars', 'done', 'dont', 'door', 'down', 'dragons', 'draw', 'dre', 'dread', 'drink', 'drive', 'dropped', 'dude', 'due', 'during', 'dying', 'each', 'earlier', 'earth', 'easier', 'effort', 'eh', 'either', 'elevate', 'else', 'elsewhere', 'email', 'employees', 'enhance', 'enjoy', 'enough', 'entered', 'entire', 'enzo', 'epicfail', 'ers', 'especially', 'even', 'ever', 'every', 'everywhere', 'ewr', 'example', 'excellent', 'excited', 'exciting', 'exp', 'experience', 'explanation', 'extra', 'extreme', 'extremely', 'ey', 'fa', 'faaaannntastic', 'fabulous', 'fail', 'failing', 'fails', 'failure', 'faith', 'fam', 'families', 'family', 'far', 'fault', 'feb', 'fee', 'feed', 'feel', 'fees', 'feesbut', 'feet', 'festivities', 'few', 'fidencio', 'field', 'filled', 'finally', 'find', 'finding', 'first', 'five', 'fix', 'fleek', 'fleets', 'flew', 'flexibility', 'flight', 'flightations', 'flighted', 'flightlation', 'flightled', 'flightly', 'flightr', 'flights', 'flightst', 'fll', 'fllgt', 'floor', 'florida', 'flown', 'flt', 'fly', 'flyers', 'flying', 'follow', 'followed', 'following', 'food', 'for', 'forever', 'forgive', 'forgiven', 'forgotmiahe', 'fortunately', 'forward', 'fran', 'frank', 'free', 'freeneversucks', 'freezing', 'frequent', 'freyasfund', 'friday', 'friend', 'friends', 'from', 'frustrated', 'ft', 'ftw', 'fuck', 'fuckin', 'fudgin', 'funeral', 'funny', 'future', 'gainesville', 'gate', 'gave', 'get', 'gets', 'getting', 'gettingbetter', 'give', 'given', 'giving', 'glad', 'gmail', 'go', 'goal', 'goes', 'going', 'gold', 'gonna', 'good', 'goodness', 'gorgeous', 'got', 'gotcha', 'gotta', 'gotten', 'grades', 'grateful', 'great', 'greatest', 'ground', 'group', 'gt', 'guess', 'guy', 'guys', 'had', 'haha', 'hahah', 'hahaha', 'half', 'handle', 'hang', 'happens', 'happy', 'happytweet', 'hard', 'has', 'hasnt', 'hasshe', 'hate', 'have', 'havent', 'having', 'hawaii', 'he', 'headed', 'hear', 'heard', 'heart', 'heel', 'hello', 'help', 'helped', 'helpful', 'helpfulness', 'helping', 'her', 'here', 'herself', 'hey', 'hi', 'highlight', 'him', 'his', 'hold', 'home', 'homegirl', 'honey', 'honor', 'hoo', 'hoot', 'hope', 'hoping', 'horse', 'hospitality', 'hot', 'hotel', 'hour', 'hours', 'how', 'hr', 'hrs', 'humphrey', 'hung', 'hungry', 'hurt', 'iad', 'ice', 'icing', 'id', 'idols', 'if', 'ignoring', 'ihop', 'ill', 'im', 'imagine', 'imjustsaying', 'impossible', 'impressed', 'imtired', 'in', 'incredible', 'industry', 'info', 'ins', 'instead', 'integration', 'internet', 'into', 'inventory', 'iphone', 'irreplaceable', 'is', 'isnt', 'issue', 'issues', 'it', 'its', 'ive', 'jacksonville', 'jb', 'jbi', 'jerks', 'jet', 'jetblue', 'jetbluemember', 'jfk', 'job', 'joke', 'juan', 'jumped', 'just', 'justdoit', 'justwantmybed', 'keep', 'keepitup', 'kept', 'kidding', 'kids', 'kind', 'kindness', 'kit', 'know', 'la', 'land', 'landed', 'landing', 'las', 'last', 'lastella', 'late', 'lauderdale', 'lax', 'least', 'leave', 'leaving', 'leavingtomm', 'left', 'legroom', 'lemme', 'let', 'letting', 'lga', 'life', 'like', 'line', 'link', 'little', 'll', 'load', 'lol', 'long', 'look', 'looking', 'looks', 'looong', 'lost', 'lots', 'louisville', 'love', 'loved', 'lovely', 'loving', 'loyalty', 'lt', 'lucky', 'luggage', 'luv', 'mad', 'made', 'maimi', 'main', 'maintained', 'maintenance', 'major', 'make', 'makes', 'making', 'manchester', 'many', 'marks', 'match', 'math', 'may', 'maybe', 'mci', 'mco', 'mdwgt', 'me', 'mean', 'means', 'mech', 'meet', 'meeting', 'meetingoutraged', 'meetings', 'mem', 'member', 'memorial', 'memories', 'mentioned', 'messed', 'mia', 'miami', 'middle', 'midterm', 'mileage', 'miles', 'milestone', 'military', 'min', 'mins', 'minus', 'minute', 'minutes', 'miss', 'missed', 'missing', 'mistake', 'mistakes', 'moodlighting', 'moodlitmonday', 'more', 'mosaic', 'most', 'moved', 'much', 'muh', 'must', 'my', 'name', 'named', 'natural', 'nc', 'need', 'needed', 'needs', 'nerves', 'never', 'neverflyusairways', 'new', 'newark', 'news', 'next', 'nice', 'nicely', 'nicest', 'night', 'nj', 'no', 'nonstop', 'noooo', 'norma', 'normally', 'not', 'notahappycustomer', 'nothanks', 'nothappy', 'nothing', 'notice', 'notifying', 'now', 'number', 'ny', 'nyc', 'of', 'off', 'offer', 'offers', 'official', 'officially', 'often', 'oh', 'ohare', 'ok', 'old', 'omaha', 'omgee', 'on', 'onboard', 'once', 'one', 'ones', 'onfleek', 'only', 'onto', 'open', 'opened', 'opens', 'opposed', 'options', 'or', 'ord', 'order', 'orlando', 'oscar', 'other', 'ots', 'our', 'ourprincess', 'out', 'outfitted', 'outstanding', 'over', 'page', 'pants', 'paris', 'part', 'party', 'pass', 'passbook', 'passed', 'passengers', 'past', 'patient', 'pay', 'peanut', 'peanuts', 'peculiar', 'people', 'perks', 'person', 'philadelphia', 'philly', 'phl', 'phoenix', 'phone', 'photos', 'phx', 'pick', 'picked', 'pics', 'picture', 'pilot', 'place', 'plane', 'planes', 'planned', 'plans', 'plat', 'play', 'please', 'pleasure', 'pls', 'plus', 'plz', 'point', 'points', 'pooch', 'poor', 'positive', 'pound', 'pr', 'praying', 'preciation', 'prefer', 'preferably', 'pressure', 'previously', 'price', 'pricing', 'priorities', 'priority', 'prize', 'probably', 'problem', 'problems', 'procedure', 'professional', 'professors', 'promos', 'properly', 'props', 'prove', 'provided', 'puerto', 'purchase', 'purifier', 'purpose', 'put', 'putting', 'qualify', 'question', 'races', 'radish', 'raeann', 'raise', 'rather', 'read', 'real', 'really', 'reason', 'rebooked', 'receiving', 'record', 'redsox', 'reflight', 'refreshed', 'refund', 'refused', 'reimbursement', 'reinstated', 'relations', 'relaxing', 'reminder', 'rental', 'rep', 'repaid', 'replacing', 'reply', 'reps', 'requested', 'requirement', 'reservation', 'reservations', 'reserved', 'response', 'restored', 'rethinking', 'return', 'rico', 'ridiculous', 'right', 'ripme', 'rn', 'roasted', 'rock', 'round', 'route', 'row', 'rt', 'rude', 'rudeness', 'runway', 'ruth', 'sac', 'sad', 'safe', 'safety', 'said', 'salted', 'same', 'san', 'sanitized', 'sass', 'sat', 'saved', 'say', 'says', 'sba', 'school', 'scott', 'sea', 'seat', 'seatac', 'seating', 'seats', 'seattlebound', 'second', 'security', 'sedholm', 'see', 'seem', 'seen', 'select', 'selected', 'semester', 'send', 'sending', 'sense', 'sent', 'serv', 'service', 'services', 'set', 'several', 'sf', 'sfo', 'share', 'sharing', 'she', 'shes', 'shhhh', 'shortage', 'should', 'shouldnt', 'shouldve', 'show', 'shut', 'sick', 'side', 'sign', 'silver', 'sincerely', 'sing', 'sister', 'site', 'sitting', 'situation', 'sjc', 'sleeping', 'slow', 'smooth', 'snacks', 'snow', 'snowstorms', 'so', 'sold', 'some', 'something', 'song', 'soon', 'sooner', 'sooo', 'sooooo', 'sounds', 'southbendinwhere', 'southwest', 'southwestairlines', 'speed', 'spoke', 'squished', 'staff', 'stahp', 'standing', 'start', 'started', 'starting', 'stat', 'static', 'status', 'steal', 'steps', 'still', 'stl', 'stolen', 'stop', 'storm', 'strand', 'stranded', 'streamline', 'stuck', 'stuckintampa', 'student', 'such', 'suck', 'suitcase', 'sunday', 'super', 'support', 'supposed', 'suprise', 'sure', 'survive', 'suspended', 'sw', 'sweet', 'swooped', 'system', 't5', 'tables', 'take', 'taken', 'taking', 'talked', 'talking', 'tallahassee', 'tampa', 'taxi', 'tea', 'team', 'technical', 'tell', 'telling', 'ten', 'terminal', 'terrific', 'test', 'texas', 'texts', 'than', 'thank', 'thanks', 'thankyou', 'that', 'thats', 'the', 'their', 'them', 'then', 'there', 'theres', 'these', 'theworst', 'they', 'theyll', 'theyve', 'thing', 'things', 'think', 'this', 'tho', 'those', 'though', 'thought', 'three', 'through', 'throughout', 'throw', 'throwing', 'thru', 'thx', 'tick', 'ticket', 'ticketits', 'tickets', 'time', 'timeliness', 'timely', 'times', 'tix', 'tke', 'to', 'today', 'toilet', 'told', 'tomoro', 'tomorrow', 'tonight', 'too', 'took', 'top', 'toss', 'totally', 'touched', 'town', 'traffic', 'train', 'transported', 'travel', 'traveling', 'travellers', 'tray', 'treat', 'treated', 'treatment', 'tried', 'trip', 'trips', 'truck', 'truebluepoints', 'truly', 'trying', 'tsa', 'turned', 'tv', 'tweet', 'tweeted', 'tweeter', 'tweeting', 'twice', 'twitter', 'two', 'unacceptable', 'unaccommodating', 'unbelievable', 'under', 'understands', 'unfortunately', 'unhelpful', 'united', 'university', 'unpredictable', 'until', 'up', 'update', 'upgrade', 'upgraded', 'uppp', 'ups', 'upsmh', 'ur', 'urgent', 'url', 'us', 'usa', 'use', 'used', 'useful', 'useless', 'user', 'vaca', 'vacation', 'vacations', 'valued', 'vanity', 'vegas', 'very', 'via', 'video', 'vino', 'virgin', 'virginamerica', 'vodkatonics', 'vote', 'voucher', 'vx', 'wait', 'waited', 'waiting', 'waived', 'waivers', 'wanna', 'wannagohome', 'want', 'wanting', 'warm', 'was', 'wasnt', 'wasteoftime', 'watching', 'way', 'we', 'weak', 'weather', 'wed', 'wedding', 'week', 'weekend', 'weekends', 'welcome', 'welcoming', 'well', 'wendi', 'went', 'were', 'weve', 'what', 'whats', 'when', 'where', 'whether', 'while', 'who', 'whoot', 'wht', 'why', 'wifi', 'will', 'willing', 'win', 'window', 'wins', 'wish', 'wit', 'with', 'without', 'won', 'wonder', 'wonderful', 'wondering', 'wont', 'woo', 'word', 'work', 'worked', 'working', 'worries', 'worse', 'worst', 'would', 'wouldnt', 'wow', 'wrench', 'write', 'ya', 'yall', 'year', 'years', 'yelled', 'yep', 'yes', 'yeseniahernandez', 'yet', 'yo', 'you', 'youdo', 'youknowyouwantto', 'your', 'youre', 'yours', 'yr'] 1329\n"
     ]
    }
   ],
   "source": [
    "print(cv1.get_feature_names(), len(cv1.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
